{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/dlnd-tf-lab/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.engine.topology import Input\n",
    "from keras.layers import MaxPooling1D,GlobalMaxPooling1D,Conv1D,Dense,Dropout,regularizers,GlobalAveragePooling1D\n",
    "from keras.models import Model,load_model\n",
    "from keras import optimizers\n",
    "import h5py # necessary for saving keras model\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate,GaussianNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "# first we need to download glove.6B according to README\n",
    "# load pretrained word embeddings:glove.6B.100d.txt\n",
    "\n",
    "filename = 'glove.6B.100d.txt' ## this file 'glove.6B.100d.txt' is composed of lines with each line containning a word and its embedding.\n",
    "\n",
    "## store words and their embeddings in separate lists, then turn embedding list to np array.\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    #with open (filename,'r') as f:\n",
    "        #emb_size = len(f.readline().strip().split(' '))-1\n",
    "    #vocab.append('unk') #load unknown words\n",
    "    #embd.append([0]*emb_size)\n",
    "    \n",
    "    file = open(filename,'r')\n",
    "    for line in file:\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "7532\n",
      "11314\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed data before vecortization\n",
    "with open(\"Preprocessed data for CNN - update.txt\",'rb') as f:\n",
    "    texts_train_ls = pickle.load(f)\n",
    "    texts_test_ls = pickle.load(f)\n",
    "    labels_train = pickle.load(f)\n",
    "    labels_test = pickle.load(f)\n",
    "print (len(texts_train_ls))\n",
    "print (len(texts_test_ls))\n",
    "print (len(labels_train))\n",
    "print (len(labels_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133927\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary of the text; elementary vectorization.\n",
    "num_words = 20000\n",
    "max_length = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words = num_words)\n",
    "tokenizer.fit_on_texts (texts_train_ls)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts_train_ls)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test_ls)\n",
    "\n",
    "data_train = pad_sequences(sequences_train,maxlen = max_length)\n",
    "data_test = pad_sequences(sequences_test,maxlen = max_length)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print (len(word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_train = to_categorical(labels_train)\n",
    "targets_test = to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build embedding matrix\n",
    "embedding_matrix = np.zeros((num_words+1,embedding_dim))\n",
    "\n",
    "for word,index in word_index.items(): # word_index start with index one, not zero.\n",
    "    if index > num_words:\n",
    "        continue\n",
    "    if word not in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        embedding_matrix[index] = embedding[vocab.index(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build embedding layer\n",
    "embedding_layer = Embedding(num_words+1,embedding_dim,weights = [embedding_matrix],input_length = data_train.shape[1],trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "sequence_input = Input(shape = (data_train.shape[1],),dtype = 'int32')\n",
    "embedded_input = embedding_layer(sequence_input)\n",
    "model_pre = Model(sequence_input,embedded_input)\n",
    "\n",
    "embedded_data_train = model_pre.predict(data_train)\n",
    "embedded_data_test = model_pre.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1 = embedded_data_test[:int(len(embedded_data_test)*0.4)]\n",
    "X_test_2 = embedded_data_test[int(len(embedded_data_test)*0.4):]\n",
    "\n",
    "y_test_1 = targets_test[:int(len(embedded_data_test)*0.4)]\n",
    "y_test_2 = targets_test[int(len(embedded_data_test)*0.4):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the original train set into a new train set and a validation set\n",
    "X_train,X_val,y_train,y_val = train_test_split(embedded_data_train,targets_train,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embedded_dataset_20000_300_TextCNN.txt\",'wb') as f:\n",
    "    pickle.dump(X_train,f)\n",
    "    pickle.dump(X_val,f)\n",
    "    pickle.dump(y_train,f)\n",
    "    pickle.dump(y_val,f)\n",
    "    \n",
    "    pickle.dump(X_test_1,f)\n",
    "    pickle.dump(X_test_2,f)\n",
    "    pickle.dump(y_test_1,f)\n",
    "    pickle.dump(y_test_2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check point. Load the embedded train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embedded_dataset_20000_300_TextCNN.txt\",'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "    X_val = pickle.load(f)\n",
    "    y_train = pickle.load(f)\n",
    "    y_val = pickle.load(f)\n",
    "   \n",
    "    X_test_1 = pickle.load(f)\n",
    "    X_test_2 = pickle.load(f)\n",
    "    y_test_1 = pickle.load(f)\n",
    "    y_test_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9051, 300, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_performance(history_list,n_epochs_per_hs):\n",
    "# history_list: list of histories\n",
    "# n_epochs_per_hs: the number of epochs per history\n",
    "    \n",
    "    plt.style.use(\"ggplot\")\n",
    "    \n",
    "    hs_keys = list(history_list[0].history.keys())\n",
    "    labels = hs_keys\n",
    "    clr = ['r','orange','green','b']\n",
    "    fig = plt.figure(figsize = (16,9))\n",
    "    n_epochs = n_epochs_per_hs * len(history_list)\n",
    "\n",
    "    for i in range(len(history_list)):\n",
    "        for j,key in enumerate(hs_keys): \n",
    "            plt.plot(range(i*n_epochs_per_hs,(i+1)*n_epochs_per_hs),history_list[i].history[key],clr[j])\n",
    "    plt.legend(labels)\n",
    "    plt.grid(True,linestyle = \"-\",color = 'k',linewidth = 0.1)\n",
    "    plt.xticks(np.arange(0,n_epochs,10))\n",
    "    plt.yticks(np.arange(0,3,0.1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a18bf7ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The following graph shows performance of the final model\n",
    "history_list = [hs_11_2,hs_11_3,hs_11_4,hs_11_5]\n",
    "n_epochs_per_hs = 40\n",
    "\n",
    "draw_performance(history_list,n_epochs_per_hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 3.6968 - acc: 0.1558 - val_loss: 2.1952 - val_acc: 0.4848\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 2.0077 - acc: 0.3868 - val_loss: 1.7329 - val_acc: 0.6310\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 1.5211 - acc: 0.5241 - val_loss: 1.4329 - val_acc: 0.7203\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 1.2681 - acc: 0.6018 - val_loss: 1.2634 - val_acc: 0.7517\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 1.1187 - acc: 0.6506 - val_loss: 1.1369 - val_acc: 0.7658\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 1.0071 - acc: 0.6847 - val_loss: 1.0403 - val_acc: 0.7945\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.9364 - acc: 0.7087 - val_loss: 0.9787 - val_acc: 0.8007\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.8654 - acc: 0.7310 - val_loss: 0.9087 - val_acc: 0.8153\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.7996 - acc: 0.7501 - val_loss: 0.8623 - val_acc: 0.8166\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.7617 - acc: 0.7626 - val_loss: 0.8226 - val_acc: 0.8299\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.7178 - acc: 0.7776 - val_loss: 0.8006 - val_acc: 0.8299\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.6885 - acc: 0.7865 - val_loss: 0.7685 - val_acc: 0.8255\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.6477 - acc: 0.8002 - val_loss: 0.7329 - val_acc: 0.8378\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.6208 - acc: 0.8096 - val_loss: 0.7078 - val_acc: 0.8356\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.5995 - acc: 0.8143 - val_loss: 0.6871 - val_acc: 0.8400\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.5882 - acc: 0.8195 - val_loss: 0.6743 - val_acc: 0.8462\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.5643 - acc: 0.8253 - val_loss: 0.6544 - val_acc: 0.8537\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.5310 - acc: 0.8352 - val_loss: 0.6371 - val_acc: 0.8422\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.5224 - acc: 0.8366 - val_loss: 0.6293 - val_acc: 0.8458\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.5016 - acc: 0.8486 - val_loss: 0.6162 - val_acc: 0.8524\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.4854 - acc: 0.8514 - val_loss: 0.5943 - val_acc: 0.8586\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.4774 - acc: 0.8531 - val_loss: 0.5896 - val_acc: 0.8577\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.4684 - acc: 0.8534 - val_loss: 0.5811 - val_acc: 0.8520\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.4502 - acc: 0.8617 - val_loss: 0.5818 - val_acc: 0.8582\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.4550 - acc: 0.8595 - val_loss: 0.5743 - val_acc: 0.8506\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.4250 - acc: 0.8655 - val_loss: 0.5564 - val_acc: 0.8555\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.4245 - acc: 0.8680 - val_loss: 0.5538 - val_acc: 0.8573\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.4075 - acc: 0.8754 - val_loss: 0.5421 - val_acc: 0.8608\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3954 - acc: 0.8779 - val_loss: 0.5406 - val_acc: 0.8586\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.3939 - acc: 0.8745 - val_loss: 0.5258 - val_acc: 0.8626\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3804 - acc: 0.8852 - val_loss: 0.5279 - val_acc: 0.8608\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3724 - acc: 0.8821 - val_loss: 0.5198 - val_acc: 0.8648\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3574 - acc: 0.8919 - val_loss: 0.5111 - val_acc: 0.8635\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3517 - acc: 0.8910 - val_loss: 0.5179 - val_acc: 0.8626\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3501 - acc: 0.8919 - val_loss: 0.5095 - val_acc: 0.8577\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3429 - acc: 0.8942 - val_loss: 0.5045 - val_acc: 0.8612\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3367 - acc: 0.8966 - val_loss: 0.5069 - val_acc: 0.8630\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.3383 - acc: 0.8957 - val_loss: 0.5027 - val_acc: 0.8652\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3319 - acc: 0.8976 - val_loss: 0.5049 - val_acc: 0.8635\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.3257 - acc: 0.9005 - val_loss: 0.5080 - val_acc: 0.8590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3842fc18>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first TextCNN model\n",
    "input_x = Input(shape = (X_train.shape[1],X_train.shape[2]),dtype = 'float32')\n",
    "\n",
    "x1 = Conv1D(64,3,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "x2 = Conv1D(64,5,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "x3 = Conv1D(64,7,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "\n",
    "x1 = GlobalMaxPooling1D()(x1)# 64 dim vector\n",
    "x2 = GlobalMaxPooling1D()(x2) # 64 dim vector\n",
    "x3 = GlobalMaxPooling1D()(x3) # 64 dim vector\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2,x3])\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='softmax')(x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "\n",
    "model = Model(input_x,preds)\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val)) #8652\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3257 - acc: 0.8988 - val_loss: 0.5077 - val_acc: 0.8559\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.3186 - acc: 0.8949 - val_loss: 0.5002 - val_acc: 0.8599\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.3095 - acc: 0.9011 - val_loss: 0.4908 - val_acc: 0.8604\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3075 - acc: 0.9043 - val_loss: 0.4977 - val_acc: 0.8617\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3021 - acc: 0.9054 - val_loss: 0.4816 - val_acc: 0.8608\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2992 - acc: 0.9064 - val_loss: 0.4902 - val_acc: 0.8568\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2904 - acc: 0.9079 - val_loss: 0.4835 - val_acc: 0.8630\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2901 - acc: 0.9061 - val_loss: 0.4807 - val_acc: 0.8586\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2887 - acc: 0.9118 - val_loss: 0.4826 - val_acc: 0.8604\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2699 - acc: 0.9148 - val_loss: 0.4800 - val_acc: 0.8617\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2593 - acc: 0.9170 - val_loss: 0.4773 - val_acc: 0.8643\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2781 - acc: 0.9153 - val_loss: 0.4763 - val_acc: 0.8639\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2715 - acc: 0.9180 - val_loss: 0.4792 - val_acc: 0.8639\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2653 - acc: 0.9154 - val_loss: 0.4694 - val_acc: 0.8604\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2755 - acc: 0.9111 - val_loss: 0.4760 - val_acc: 0.8590\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2741 - acc: 0.9166 - val_loss: 0.4724 - val_acc: 0.8590\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2655 - acc: 0.9165 - val_loss: 0.4767 - val_acc: 0.8626\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2560 - acc: 0.9198 - val_loss: 0.4687 - val_acc: 0.8661\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2509 - acc: 0.9209 - val_loss: 0.4603 - val_acc: 0.8652\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2687 - acc: 0.9126 - val_loss: 0.4730 - val_acc: 0.8674\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2486 - acc: 0.9208 - val_loss: 0.4766 - val_acc: 0.8643\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2519 - acc: 0.9232 - val_loss: 0.4638 - val_acc: 0.8696\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2485 - acc: 0.9205 - val_loss: 0.4704 - val_acc: 0.8617\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2481 - acc: 0.9203 - val_loss: 0.4710 - val_acc: 0.8643\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2383 - acc: 0.9230 - val_loss: 0.4837 - val_acc: 0.8568\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2479 - acc: 0.9214 - val_loss: 0.4705 - val_acc: 0.8621\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2453 - acc: 0.9228 - val_loss: 0.4636 - val_acc: 0.8648\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2469 - acc: 0.9249 - val_loss: 0.4700 - val_acc: 0.8639\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2256 - acc: 0.9274 - val_loss: 0.4687 - val_acc: 0.8657\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2264 - acc: 0.9281 - val_loss: 0.4692 - val_acc: 0.8608\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2199 - acc: 0.9321 - val_loss: 0.4709 - val_acc: 0.8608\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2266 - acc: 0.9313 - val_loss: 0.4732 - val_acc: 0.8670\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2191 - acc: 0.9298 - val_loss: 0.4668 - val_acc: 0.8688\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2232 - acc: 0.9317 - val_loss: 0.4685 - val_acc: 0.8670\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2308 - acc: 0.9264 - val_loss: 0.4656 - val_acc: 0.8679\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2251 - acc: 0.9267 - val_loss: 0.4686 - val_acc: 0.8639\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2251 - acc: 0.9275 - val_loss: 0.4625 - val_acc: 0.8701\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.2149 - acc: 0.9315 - val_loss: 0.4630 - val_acc: 0.8674\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2299 - acc: 0.9265 - val_loss: 0.4695 - val_acc: 0.8652\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2207 - acc: 0.9318 - val_loss: 0.4812 - val_acc: 0.8621\n"
     ]
    }
   ],
   "source": [
    "hs_2 = model.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#870137th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2021 - acc: 0.9372 - val_loss: 0.4651 - val_acc: 0.8630\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2227 - acc: 0.9295 - val_loss: 0.4685 - val_acc: 0.8621\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2146 - acc: 0.9332 - val_loss: 0.4769 - val_acc: 0.8648\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2011 - acc: 0.9363 - val_loss: 0.4620 - val_acc: 0.8661\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2016 - acc: 0.9357 - val_loss: 0.4596 - val_acc: 0.8679\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2040 - acc: 0.9356 - val_loss: 0.4563 - val_acc: 0.8657\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2081 - acc: 0.9329 - val_loss: 0.4631 - val_acc: 0.8670\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2115 - acc: 0.9325 - val_loss: 0.4653 - val_acc: 0.8590\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2042 - acc: 0.9347 - val_loss: 0.4681 - val_acc: 0.8648\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2071 - acc: 0.9319 - val_loss: 0.4749 - val_acc: 0.8612\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.2024 - acc: 0.9392 - val_loss: 0.4611 - val_acc: 0.8661\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2046 - acc: 0.9333 - val_loss: 0.4728 - val_acc: 0.8608\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.2002 - acc: 0.9361 - val_loss: 0.4640 - val_acc: 0.8648\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1948 - acc: 0.9382 - val_loss: 0.4614 - val_acc: 0.8635\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1978 - acc: 0.9382 - val_loss: 0.4660 - val_acc: 0.8630\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2096 - acc: 0.9363 - val_loss: 0.4616 - val_acc: 0.8630\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1863 - acc: 0.9386 - val_loss: 0.4720 - val_acc: 0.8639\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1898 - acc: 0.9395 - val_loss: 0.4756 - val_acc: 0.8617\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1948 - acc: 0.9388 - val_loss: 0.4635 - val_acc: 0.8612\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1853 - acc: 0.9396 - val_loss: 0.4940 - val_acc: 0.8533\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1833 - acc: 0.9437 - val_loss: 0.4679 - val_acc: 0.8661\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1864 - acc: 0.9435 - val_loss: 0.4664 - val_acc: 0.8670\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1913 - acc: 0.9382 - val_loss: 0.4701 - val_acc: 0.8670\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1862 - acc: 0.9416 - val_loss: 0.4786 - val_acc: 0.8652\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1786 - acc: 0.9450 - val_loss: 0.4755 - val_acc: 0.8701\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1896 - acc: 0.9395 - val_loss: 0.4873 - val_acc: 0.8582\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1904 - acc: 0.9412 - val_loss: 0.4806 - val_acc: 0.8612\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1860 - acc: 0.9416 - val_loss: 0.4755 - val_acc: 0.8630\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1817 - acc: 0.9445 - val_loss: 0.4751 - val_acc: 0.8670\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1708 - acc: 0.9465 - val_loss: 0.4723 - val_acc: 0.8657\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1845 - acc: 0.9452 - val_loss: 0.4916 - val_acc: 0.8595\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1814 - acc: 0.9423 - val_loss: 0.4779 - val_acc: 0.8617\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1845 - acc: 0.9404 - val_loss: 0.4846 - val_acc: 0.8608\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1937 - acc: 0.9393 - val_loss: 0.4892 - val_acc: 0.8648\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1838 - acc: 0.9419 - val_loss: 0.4874 - val_acc: 0.8604\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1817 - acc: 0.9407 - val_loss: 0.4810 - val_acc: 0.8670\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1732 - acc: 0.9462 - val_loss: 0.4841 - val_acc: 0.8661\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1847 - acc: 0.9414 - val_loss: 0.4812 - val_acc: 0.8608\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1836 - acc: 0.9427 - val_loss: 0.4693 - val_acc: 0.8621\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1775 - acc: 0.9444 - val_loss: 0.4728 - val_acc: 0.8604\n"
     ]
    }
   ],
   "source": [
    "hs_3 = model.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#8701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1799 - acc: 0.9469 - val_loss: 0.4752 - val_acc: 0.8621\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1592 - acc: 0.9501 - val_loss: 0.4694 - val_acc: 0.8617\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1566 - acc: 0.9519 - val_loss: 0.4719 - val_acc: 0.8621\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1420 - acc: 0.9568 - val_loss: 0.4681 - val_acc: 0.8661\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1550 - acc: 0.9522 - val_loss: 0.4711 - val_acc: 0.8679\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1431 - acc: 0.9576 - val_loss: 0.4753 - val_acc: 0.8639\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1543 - acc: 0.9526 - val_loss: 0.4671 - val_acc: 0.8688\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1435 - acc: 0.9547 - val_loss: 0.4729 - val_acc: 0.8688\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1375 - acc: 0.9569 - val_loss: 0.4728 - val_acc: 0.8683\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1422 - acc: 0.9561 - val_loss: 0.4775 - val_acc: 0.8670\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1403 - acc: 0.9559 - val_loss: 0.4787 - val_acc: 0.8657\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1483 - acc: 0.9562 - val_loss: 0.4817 - val_acc: 0.8648\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1430 - acc: 0.9556 - val_loss: 0.4838 - val_acc: 0.8626\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1308 - acc: 0.9597 - val_loss: 0.4747 - val_acc: 0.8670\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1450 - acc: 0.9549 - val_loss: 0.4767 - val_acc: 0.8688\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1353 - acc: 0.9572 - val_loss: 0.4756 - val_acc: 0.8692\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1244 - acc: 0.9608 - val_loss: 0.4822 - val_acc: 0.8612\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1292 - acc: 0.9609 - val_loss: 0.4824 - val_acc: 0.8630\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1324 - acc: 0.9589 - val_loss: 0.4744 - val_acc: 0.8665\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1248 - acc: 0.9618 - val_loss: 0.4759 - val_acc: 0.8670\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1249 - acc: 0.9629 - val_loss: 0.4703 - val_acc: 0.8679\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 2098s 232ms/step - loss: 0.1323 - acc: 0.9609 - val_loss: 0.4746 - val_acc: 0.8661\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 42s 5ms/step - loss: 0.1335 - acc: 0.9598 - val_loss: 0.4708 - val_acc: 0.8683\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1241 - acc: 0.9629 - val_loss: 0.4677 - val_acc: 0.8696\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1195 - acc: 0.9634 - val_loss: 0.4758 - val_acc: 0.8679\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1281 - acc: 0.9599 - val_loss: 0.4762 - val_acc: 0.8670\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1247 - acc: 0.9641 - val_loss: 0.4749 - val_acc: 0.8719\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1174 - acc: 0.9632 - val_loss: 0.4755 - val_acc: 0.8719\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1301 - acc: 0.9595 - val_loss: 0.4816 - val_acc: 0.8727\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1253 - acc: 0.9621 - val_loss: 0.4747 - val_acc: 0.8670\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1110 - acc: 0.9657 - val_loss: 0.4795 - val_acc: 0.8670\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1177 - acc: 0.9640 - val_loss: 0.4813 - val_acc: 0.8665\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1261 - acc: 0.9617 - val_loss: 0.4792 - val_acc: 0.8688\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1193 - acc: 0.9640 - val_loss: 0.4771 - val_acc: 0.8701\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1226 - acc: 0.9643 - val_loss: 0.4860 - val_acc: 0.8674\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1111 - acc: 0.9633 - val_loss: 0.4856 - val_acc: 0.8683\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1238 - acc: 0.9633 - val_loss: 0.4887 - val_acc: 0.8670\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1184 - acc: 0.9650 - val_loss: 0.4853 - val_acc: 0.8692\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1074 - acc: 0.9663 - val_loss: 0.4868 - val_acc: 0.8643\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1161 - acc: 0.9637 - val_loss: 0.4876 - val_acc: 0.8670\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "hs_4 = model.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"TextCNN_model_1\")#val_acc:8727(149th epoch),val_loss:4563(85th epoch)~49+; tr_acc:9717(below,not saved.epoch 196),tr_loss:0944(below,not saved.epoch 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1087 - acc: 0.9673 - val_loss: 0.4866 - val_acc: 0.8679\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1165 - acc: 0.9633 - val_loss: 0.4905 - val_acc: 0.8670\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1155 - acc: 0.9643 - val_loss: 0.4881 - val_acc: 0.8657\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1072 - acc: 0.9678 - val_loss: 0.4904 - val_acc: 0.8679\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1156 - acc: 0.9656 - val_loss: 0.4872 - val_acc: 0.8688\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1234 - acc: 0.9631 - val_loss: 0.4887 - val_acc: 0.8674\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 36s 4ms/step - loss: 0.1190 - acc: 0.9632 - val_loss: 0.4879 - val_acc: 0.8639\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1200 - acc: 0.9609 - val_loss: 0.4847 - val_acc: 0.8692\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1167 - acc: 0.9633 - val_loss: 0.4871 - val_acc: 0.8701\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1203 - acc: 0.9627 - val_loss: 0.4850 - val_acc: 0.8727\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1151 - acc: 0.9657 - val_loss: 0.4890 - val_acc: 0.8670\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1182 - acc: 0.9633 - val_loss: 0.4851 - val_acc: 0.8670\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1087 - acc: 0.9661 - val_loss: 0.4889 - val_acc: 0.8692\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1122 - acc: 0.9660 - val_loss: 0.4858 - val_acc: 0.8710\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1114 - acc: 0.9654 - val_loss: 0.4914 - val_acc: 0.8692\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1148 - acc: 0.9656 - val_loss: 0.4928 - val_acc: 0.8665\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1131 - acc: 0.9672 - val_loss: 0.4975 - val_acc: 0.8648\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1124 - acc: 0.9687 - val_loss: 0.4943 - val_acc: 0.8652\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1142 - acc: 0.9663 - val_loss: 0.4879 - val_acc: 0.8701\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 41s 4ms/step - loss: 0.1122 - acc: 0.9660 - val_loss: 0.4977 - val_acc: 0.8657\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1131 - acc: 0.9649 - val_loss: 0.4943 - val_acc: 0.8639\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1123 - acc: 0.9675 - val_loss: 0.4872 - val_acc: 0.8683\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1104 - acc: 0.9650 - val_loss: 0.4950 - val_acc: 0.8648\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1077 - acc: 0.9660 - val_loss: 0.4933 - val_acc: 0.8688\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1045 - acc: 0.9690 - val_loss: 0.4877 - val_acc: 0.8657\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1099 - acc: 0.9673 - val_loss: 0.4900 - val_acc: 0.8670\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1142 - acc: 0.9642 - val_loss: 0.4775 - val_acc: 0.8705\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1069 - acc: 0.9666 - val_loss: 0.4908 - val_acc: 0.8674\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1126 - acc: 0.9669 - val_loss: 0.4823 - val_acc: 0.8710\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1032 - acc: 0.9687 - val_loss: 0.4892 - val_acc: 0.8696\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1058 - acc: 0.9669 - val_loss: 0.4948 - val_acc: 0.8652\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1064 - acc: 0.9677 - val_loss: 0.4888 - val_acc: 0.8665\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1088 - acc: 0.9656 - val_loss: 0.4900 - val_acc: 0.8657\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1110 - acc: 0.9674 - val_loss: 0.4910 - val_acc: 0.8670\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1032 - acc: 0.9699 - val_loss: 0.4887 - val_acc: 0.8692\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0944 - acc: 0.9717 - val_loss: 0.4958 - val_acc: 0.8696\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1084 - acc: 0.9676 - val_loss: 0.4911 - val_acc: 0.8692\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.1126 - acc: 0.9687 - val_loss: 0.5025 - val_acc: 0.8665\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 3182s 352ms/step - loss: 0.1124 - acc: 0.9678 - val_loss: 0.4892 - val_acc: 0.8665\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.0995 - acc: 0.9708 - val_loss: 0.4974 - val_acc: 0.8701\n"
     ]
    }
   ],
   "source": [
    "hs_5 = model.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First update --- introduce GaussianNoise layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 2.9817 - acc: 0.2038 - val_loss: 2.0621 - val_acc: 0.5524\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 42s 5ms/step - loss: 1.8594 - acc: 0.4259 - val_loss: 1.6282 - val_acc: 0.6571\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 1.4610 - acc: 0.5438 - val_loss: 1.3812 - val_acc: 0.7313\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 41s 4ms/step - loss: 1.2497 - acc: 0.6088 - val_loss: 1.2195 - val_acc: 0.7494\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 1.1031 - acc: 0.6577 - val_loss: 1.1234 - val_acc: 0.7609\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.9875 - acc: 0.6895 - val_loss: 1.0152 - val_acc: 0.7857\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.9242 - acc: 0.7113 - val_loss: 0.9465 - val_acc: 0.8007\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.8528 - acc: 0.7321 - val_loss: 0.9028 - val_acc: 0.8016\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.7914 - acc: 0.7504 - val_loss: 0.8528 - val_acc: 0.8020\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.7530 - acc: 0.7684 - val_loss: 0.8123 - val_acc: 0.8246\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.7081 - acc: 0.7783 - val_loss: 0.7789 - val_acc: 0.8347\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.6672 - acc: 0.7922 - val_loss: 0.7525 - val_acc: 0.8343\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.6644 - acc: 0.7894 - val_loss: 0.7346 - val_acc: 0.8347\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.6221 - acc: 0.8037 - val_loss: 0.7136 - val_acc: 0.8436\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 43s 5ms/step - loss: 0.6009 - acc: 0.8117 - val_loss: 0.6824 - val_acc: 0.8453\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.5806 - acc: 0.8207 - val_loss: 0.6678 - val_acc: 0.8462\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.5720 - acc: 0.8219 - val_loss: 0.6590 - val_acc: 0.8471\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.5516 - acc: 0.8279 - val_loss: 0.6361 - val_acc: 0.8511\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.5129 - acc: 0.8391 - val_loss: 0.6215 - val_acc: 0.8484\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.5023 - acc: 0.8472 - val_loss: 0.6148 - val_acc: 0.8493\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4937 - acc: 0.8448 - val_loss: 0.5998 - val_acc: 0.8546\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4859 - acc: 0.8516 - val_loss: 0.5889 - val_acc: 0.8467\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4751 - acc: 0.8463 - val_loss: 0.5827 - val_acc: 0.8546\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4496 - acc: 0.8585 - val_loss: 0.5706 - val_acc: 0.8542\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4336 - acc: 0.8642 - val_loss: 0.5635 - val_acc: 0.8489\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4227 - acc: 0.8675 - val_loss: 0.5488 - val_acc: 0.8555\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4135 - acc: 0.8697 - val_loss: 0.5441 - val_acc: 0.8586\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4014 - acc: 0.8726 - val_loss: 0.5428 - val_acc: 0.8573\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.4014 - acc: 0.8745 - val_loss: 0.5326 - val_acc: 0.8559\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3890 - acc: 0.8789 - val_loss: 0.5287 - val_acc: 0.8582\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3683 - acc: 0.8803 - val_loss: 0.5214 - val_acc: 0.8608\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3718 - acc: 0.8773 - val_loss: 0.5162 - val_acc: 0.8665\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3655 - acc: 0.8849 - val_loss: 0.5160 - val_acc: 0.8586\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3585 - acc: 0.8854 - val_loss: 0.5037 - val_acc: 0.8648\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3609 - acc: 0.8874 - val_loss: 0.5085 - val_acc: 0.8630\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3445 - acc: 0.8908 - val_loss: 0.4967 - val_acc: 0.8657\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3384 - acc: 0.8954 - val_loss: 0.5039 - val_acc: 0.8626\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3326 - acc: 0.8951 - val_loss: 0.4917 - val_acc: 0.8679\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.3256 - acc: 0.8975 - val_loss: 0.4894 - val_acc: 0.8679\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.3293 - acc: 0.8943 - val_loss: 0.4855 - val_acc: 0.8648\n"
     ]
    }
   ],
   "source": [
    "# second textCNN model\n",
    "\n",
    "in_x = Input(shape = (X_train.shape[1],X_train.shape[2]),dtype = 'float32')\n",
    "\n",
    "input_x = GaussianNoise(stddev = 0.01)(in_x)\n",
    "\n",
    "x1 = Conv1D(64,3,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "x2 = Conv1D(64,5,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "x3 = Conv1D(64,7,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "\n",
    "x1 = GlobalMaxPooling1D()(x1)# 64 dim vector\n",
    "x2 = GlobalMaxPooling1D()(x2) # 64 dim vector\n",
    "x3 = GlobalMaxPooling1D()(x3) # 64 dim vector\n",
    "\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2,x3])\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='softmax')(x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "\n",
    "model_i = Model(in_x,preds)\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_i.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "hs_4_1 = model_i.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3289 - acc: 0.8988 - val_loss: 0.4922 - val_acc: 0.8621\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3207 - acc: 0.9006 - val_loss: 0.4850 - val_acc: 0.8608\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.3012 - acc: 0.9052 - val_loss: 0.4793 - val_acc: 0.8630\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2979 - acc: 0.9059 - val_loss: 0.4791 - val_acc: 0.8643\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2836 - acc: 0.9062 - val_loss: 0.4716 - val_acc: 0.8688\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2956 - acc: 0.9050 - val_loss: 0.4772 - val_acc: 0.8657\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2840 - acc: 0.9101 - val_loss: 0.4741 - val_acc: 0.8670\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2880 - acc: 0.9098 - val_loss: 0.4664 - val_acc: 0.8639\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2912 - acc: 0.9088 - val_loss: 0.4728 - val_acc: 0.8639\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2950 - acc: 0.9064 - val_loss: 0.4659 - val_acc: 0.8639\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2766 - acc: 0.9111 - val_loss: 0.4723 - val_acc: 0.8648\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2697 - acc: 0.9121 - val_loss: 0.4673 - val_acc: 0.8696\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2810 - acc: 0.9108 - val_loss: 0.4790 - val_acc: 0.8621\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2720 - acc: 0.9160 - val_loss: 0.4602 - val_acc: 0.8683\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2530 - acc: 0.9229 - val_loss: 0.4597 - val_acc: 0.8692\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2620 - acc: 0.9172 - val_loss: 0.4641 - val_acc: 0.8727\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2689 - acc: 0.9154 - val_loss: 0.4565 - val_acc: 0.8719\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2653 - acc: 0.9128 - val_loss: 0.4592 - val_acc: 0.8714\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2572 - acc: 0.9220 - val_loss: 0.4664 - val_acc: 0.8670\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2505 - acc: 0.9210 - val_loss: 0.4547 - val_acc: 0.8710\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2537 - acc: 0.9189 - val_loss: 0.4617 - val_acc: 0.8696\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2412 - acc: 0.9208 - val_loss: 0.4579 - val_acc: 0.8701\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2477 - acc: 0.9238 - val_loss: 0.4574 - val_acc: 0.8688\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2465 - acc: 0.9241 - val_loss: 0.4532 - val_acc: 0.8719\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2330 - acc: 0.9249 - val_loss: 0.4559 - val_acc: 0.8705\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 42s 5ms/step - loss: 0.2443 - acc: 0.9253 - val_loss: 0.4497 - val_acc: 0.8719\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2399 - acc: 0.9252 - val_loss: 0.4448 - val_acc: 0.8767\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.2403 - acc: 0.9243 - val_loss: 0.4505 - val_acc: 0.8723\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.2208 - acc: 0.9293 - val_loss: 0.4445 - val_acc: 0.8772\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.2265 - acc: 0.9248 - val_loss: 0.4517 - val_acc: 0.8719\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.2264 - acc: 0.9262 - val_loss: 0.4575 - val_acc: 0.8719\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.2255 - acc: 0.9297 - val_loss: 0.4521 - val_acc: 0.8696\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.2240 - acc: 0.9317 - val_loss: 0.4528 - val_acc: 0.8732\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.2121 - acc: 0.9302 - val_loss: 0.4438 - val_acc: 0.8736\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 43s 5ms/step - loss: 0.2120 - acc: 0.9325 - val_loss: 0.4518 - val_acc: 0.8758\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 42s 5ms/step - loss: 0.2226 - acc: 0.9281 - val_loss: 0.4607 - val_acc: 0.8736\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2125 - acc: 0.9327 - val_loss: 0.4550 - val_acc: 0.8745\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2221 - acc: 0.9303 - val_loss: 0.4488 - val_acc: 0.8741\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2105 - acc: 0.9350 - val_loss: 0.4503 - val_acc: 0.8705\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2202 - acc: 0.9280 - val_loss: 0.4575 - val_acc: 0.8679\n"
     ]
    }
   ],
   "source": [
    "hs_4_2 = model_i.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#val_acc =8772(29th epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2287 - acc: 0.9282 - val_loss: 0.4518 - val_acc: 0.8714\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 41s 4ms/step - loss: 0.2229 - acc: 0.9292 - val_loss: 0.4583 - val_acc: 0.8657\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.2188 - acc: 0.9300 - val_loss: 0.4569 - val_acc: 0.8727\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2085 - acc: 0.9353 - val_loss: 0.4607 - val_acc: 0.8710\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2064 - acc: 0.9357 - val_loss: 0.4524 - val_acc: 0.8745\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2020 - acc: 0.9371 - val_loss: 0.4586 - val_acc: 0.8719\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2070 - acc: 0.9367 - val_loss: 0.4622 - val_acc: 0.8714\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.2075 - acc: 0.9356 - val_loss: 0.4562 - val_acc: 0.8736\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.2034 - acc: 0.9360 - val_loss: 0.4630 - val_acc: 0.8701\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2005 - acc: 0.9379 - val_loss: 0.4668 - val_acc: 0.8727\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1910 - acc: 0.9401 - val_loss: 0.4649 - val_acc: 0.8688\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1955 - acc: 0.9440 - val_loss: 0.4683 - val_acc: 0.8643\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1983 - acc: 0.9366 - val_loss: 0.4642 - val_acc: 0.8719\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1939 - acc: 0.9389 - val_loss: 0.4621 - val_acc: 0.8727\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1985 - acc: 0.9364 - val_loss: 0.4658 - val_acc: 0.8727\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1856 - acc: 0.9411 - val_loss: 0.4627 - val_acc: 0.8705\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1895 - acc: 0.9372 - val_loss: 0.4675 - val_acc: 0.8727\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1893 - acc: 0.9388 - val_loss: 0.4595 - val_acc: 0.8688\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1848 - acc: 0.9435 - val_loss: 0.4687 - val_acc: 0.8723\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1867 - acc: 0.9431 - val_loss: 0.4654 - val_acc: 0.8670\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1987 - acc: 0.9402 - val_loss: 0.4604 - val_acc: 0.8741\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1887 - acc: 0.9428 - val_loss: 0.4767 - val_acc: 0.8692\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.2022 - acc: 0.9365 - val_loss: 0.4581 - val_acc: 0.8710\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1868 - acc: 0.9422 - val_loss: 0.4680 - val_acc: 0.8723\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1702 - acc: 0.9469 - val_loss: 0.4651 - val_acc: 0.8705\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1869 - acc: 0.9419 - val_loss: 0.4595 - val_acc: 0.8767\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1922 - acc: 0.9387 - val_loss: 0.4611 - val_acc: 0.8710\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1858 - acc: 0.9418 - val_loss: 0.4603 - val_acc: 0.8719\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1866 - acc: 0.9431 - val_loss: 0.4619 - val_acc: 0.8745\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1811 - acc: 0.9429 - val_loss: 0.4543 - val_acc: 0.8789\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1867 - acc: 0.9424 - val_loss: 0.4572 - val_acc: 0.8736\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1785 - acc: 0.9439 - val_loss: 0.4634 - val_acc: 0.8732\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1808 - acc: 0.9432 - val_loss: 0.4733 - val_acc: 0.8696\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1741 - acc: 0.9452 - val_loss: 0.4751 - val_acc: 0.8701\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1751 - acc: 0.9459 - val_loss: 0.4673 - val_acc: 0.8723\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1769 - acc: 0.9437 - val_loss: 0.4727 - val_acc: 0.8745\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1755 - acc: 0.9466 - val_loss: 0.4745 - val_acc: 0.8705\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1717 - acc: 0.9474 - val_loss: 0.4673 - val_acc: 0.8688\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1691 - acc: 0.9472 - val_loss: 0.4715 - val_acc: 0.8714\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1666 - acc: 0.9490 - val_loss: 0.4758 - val_acc: 0.8696\n"
     ]
    }
   ],
   "source": [
    "hs_4_3 = model_i.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#val_acc:8789(30th ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_i.save(\"Text_CNN_option_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.1691 - acc: 0.9463 - val_loss: 0.4687 - val_acc: 0.8705\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1553 - acc: 0.9517 - val_loss: 0.4654 - val_acc: 0.8701\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1535 - acc: 0.9514 - val_loss: 0.4771 - val_acc: 0.8710\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1558 - acc: 0.9528 - val_loss: 0.4640 - val_acc: 0.8745\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1476 - acc: 0.9553 - val_loss: 0.4708 - val_acc: 0.8701\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1418 - acc: 0.9574 - val_loss: 0.4692 - val_acc: 0.8705\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1413 - acc: 0.9546 - val_loss: 0.4744 - val_acc: 0.8692\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1352 - acc: 0.9562 - val_loss: 0.4650 - val_acc: 0.8714\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1401 - acc: 0.9546 - val_loss: 0.4728 - val_acc: 0.8696\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1263 - acc: 0.9592 - val_loss: 0.4713 - val_acc: 0.8705\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1387 - acc: 0.9580 - val_loss: 0.4742 - val_acc: 0.8710\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1402 - acc: 0.9561 - val_loss: 0.4720 - val_acc: 0.8719\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1334 - acc: 0.9578 - val_loss: 0.4691 - val_acc: 0.8723\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1271 - acc: 0.9616 - val_loss: 0.4661 - val_acc: 0.8719\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1248 - acc: 0.9612 - val_loss: 0.4665 - val_acc: 0.8745\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1333 - acc: 0.9576 - val_loss: 0.4707 - val_acc: 0.8736\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1283 - acc: 0.9586 - val_loss: 0.4598 - val_acc: 0.8772\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1295 - acc: 0.9598 - val_loss: 0.4640 - val_acc: 0.8794\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1283 - acc: 0.9612 - val_loss: 0.4645 - val_acc: 0.8754\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1219 - acc: 0.9623 - val_loss: 0.4739 - val_acc: 0.8758\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1326 - acc: 0.9580 - val_loss: 0.4658 - val_acc: 0.8776\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1269 - acc: 0.9616 - val_loss: 0.4624 - val_acc: 0.8780\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1194 - acc: 0.9625 - val_loss: 0.4615 - val_acc: 0.8825\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1159 - acc: 0.9657 - val_loss: 0.4682 - val_acc: 0.8758\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1187 - acc: 0.9641 - val_loss: 0.4708 - val_acc: 0.8736\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1332 - acc: 0.9598 - val_loss: 0.4784 - val_acc: 0.8688\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 42s 5ms/step - loss: 0.1222 - acc: 0.9633 - val_loss: 0.4672 - val_acc: 0.8741\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1168 - acc: 0.9660 - val_loss: 0.4651 - val_acc: 0.8719\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1207 - acc: 0.9669 - val_loss: 0.4686 - val_acc: 0.8732\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1281 - acc: 0.9599 - val_loss: 0.4745 - val_acc: 0.8780\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1208 - acc: 0.9588 - val_loss: 0.4714 - val_acc: 0.8811\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 41s 4ms/step - loss: 0.1225 - acc: 0.9624 - val_loss: 0.4711 - val_acc: 0.8789\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 42s 5ms/step - loss: 0.1160 - acc: 0.9649 - val_loss: 0.4754 - val_acc: 0.8749\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1106 - acc: 0.9665 - val_loss: 0.4755 - val_acc: 0.8758\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1234 - acc: 0.9641 - val_loss: 0.4727 - val_acc: 0.8776\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1159 - acc: 0.9680 - val_loss: 0.4688 - val_acc: 0.8807\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1182 - acc: 0.9617 - val_loss: 0.4752 - val_acc: 0.8741\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1179 - acc: 0.9645 - val_loss: 0.4772 - val_acc: 0.8763\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1192 - acc: 0.9628 - val_loss: 0.4729 - val_acc: 0.8780\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1170 - acc: 0.9641 - val_loss: 0.4868 - val_acc: 0.8692\n"
     ]
    }
   ],
   "source": [
    "model_i = load_model(\"Text_CNN_option_4\")\n",
    "optimizer = optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_i.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "hs_4_4 = model_i.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_i.save(\"Text_CNN_option_4_2nd\")#val_acc 8825 143th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1154 - acc: 0.9662 - val_loss: 0.4816 - val_acc: 0.8767\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1023 - acc: 0.9690 - val_loss: 0.4803 - val_acc: 0.8780\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1060 - acc: 0.9697 - val_loss: 0.4766 - val_acc: 0.8749\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1093 - acc: 0.9662 - val_loss: 0.4732 - val_acc: 0.8736\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1026 - acc: 0.9701 - val_loss: 0.4731 - val_acc: 0.8754\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0989 - acc: 0.9705 - val_loss: 0.4773 - val_acc: 0.8745\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.1042 - acc: 0.9669 - val_loss: 0.4740 - val_acc: 0.8754\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.0977 - acc: 0.9696 - val_loss: 0.4753 - val_acc: 0.8736\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 42s 5ms/step - loss: 0.1070 - acc: 0.9674 - val_loss: 0.4779 - val_acc: 0.8754\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1056 - acc: 0.9685 - val_loss: 0.4789 - val_acc: 0.8749\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.1060 - acc: 0.9670 - val_loss: 0.4829 - val_acc: 0.8763\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0980 - acc: 0.9707 - val_loss: 0.4811 - val_acc: 0.8754\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1074 - acc: 0.9673 - val_loss: 0.4808 - val_acc: 0.8758\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1018 - acc: 0.9702 - val_loss: 0.4814 - val_acc: 0.8794\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1053 - acc: 0.9693 - val_loss: 0.4819 - val_acc: 0.8780\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1039 - acc: 0.9691 - val_loss: 0.4849 - val_acc: 0.8785\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0938 - acc: 0.9704 - val_loss: 0.4804 - val_acc: 0.8741\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 304s 34ms/step - loss: 0.0932 - acc: 0.9703 - val_loss: 0.4790 - val_acc: 0.8776\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 43s 5ms/step - loss: 0.1019 - acc: 0.9685 - val_loss: 0.4811 - val_acc: 0.8767\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0985 - acc: 0.9692 - val_loss: 0.4796 - val_acc: 0.8758\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0995 - acc: 0.9703 - val_loss: 0.4804 - val_acc: 0.8754\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1032 - acc: 0.9704 - val_loss: 0.4805 - val_acc: 0.8763\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0963 - acc: 0.9709 - val_loss: 0.4783 - val_acc: 0.8754\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1071 - acc: 0.9682 - val_loss: 0.4775 - val_acc: 0.8780\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0961 - acc: 0.9695 - val_loss: 0.4799 - val_acc: 0.8767\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.1010 - acc: 0.9716 - val_loss: 0.4847 - val_acc: 0.8749\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0926 - acc: 0.9741 - val_loss: 0.4840 - val_acc: 0.8780\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0998 - acc: 0.9702 - val_loss: 0.4848 - val_acc: 0.8767\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0881 - acc: 0.9730 - val_loss: 0.4846 - val_acc: 0.8789\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 863s 95ms/step - loss: 0.0897 - acc: 0.9737 - val_loss: 0.4834 - val_acc: 0.8776\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 41s 4ms/step - loss: 0.0943 - acc: 0.9702 - val_loss: 0.4877 - val_acc: 0.8772\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.0935 - acc: 0.9733 - val_loss: 0.4804 - val_acc: 0.8772\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0972 - acc: 0.9703 - val_loss: 0.4792 - val_acc: 0.8749\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0925 - acc: 0.9740 - val_loss: 0.4768 - val_acc: 0.8780\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.1004 - acc: 0.9716 - val_loss: 0.4767 - val_acc: 0.8794\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0937 - acc: 0.9703 - val_loss: 0.4818 - val_acc: 0.8794\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.0880 - acc: 0.9744 - val_loss: 0.4816 - val_acc: 0.8767\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.1008 - acc: 0.9684 - val_loss: 0.4826 - val_acc: 0.8776\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.0914 - acc: 0.9707 - val_loss: 0.4808 - val_acc: 0.8785\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0971 - acc: 0.9709 - val_loss: 0.4798 - val_acc: 0.8785\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.0002, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_i.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "hs_4_5 = model_i.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0971 - acc: 0.9716 - val_loss: 0.4821 - val_acc: 0.8754\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 41s 5ms/step - loss: 0.0946 - acc: 0.9743 - val_loss: 0.4823 - val_acc: 0.8776\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0917 - acc: 0.9723 - val_loss: 0.4844 - val_acc: 0.8767\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 40s 4ms/step - loss: 0.0894 - acc: 0.9715 - val_loss: 0.4826 - val_acc: 0.8798\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0944 - acc: 0.9732 - val_loss: 0.4847 - val_acc: 0.8785\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0858 - acc: 0.9760 - val_loss: 0.4828 - val_acc: 0.8776\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0924 - acc: 0.9720 - val_loss: 0.4797 - val_acc: 0.8772\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0883 - acc: 0.9729 - val_loss: 0.4802 - val_acc: 0.8763\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0868 - acc: 0.9735 - val_loss: 0.4829 - val_acc: 0.8807\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0929 - acc: 0.9743 - val_loss: 0.4808 - val_acc: 0.8763\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0960 - acc: 0.9705 - val_loss: 0.4797 - val_acc: 0.8789\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0913 - acc: 0.9739 - val_loss: 0.4806 - val_acc: 0.8772\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0864 - acc: 0.9734 - val_loss: 0.4836 - val_acc: 0.8767\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0943 - acc: 0.9729 - val_loss: 0.4825 - val_acc: 0.8794\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0927 - acc: 0.9733 - val_loss: 0.4800 - val_acc: 0.8763\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0904 - acc: 0.9730 - val_loss: 0.4795 - val_acc: 0.8780\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0934 - acc: 0.9707 - val_loss: 0.4814 - val_acc: 0.8776\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0909 - acc: 0.9715 - val_loss: 0.4829 - val_acc: 0.8780\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0893 - acc: 0.9733 - val_loss: 0.4823 - val_acc: 0.8798\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0779 - acc: 0.9756 - val_loss: 0.4829 - val_acc: 0.8749\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0921 - acc: 0.9722 - val_loss: 0.4836 - val_acc: 0.8785\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0764 - acc: 0.9777 - val_loss: 0.4803 - val_acc: 0.8785\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 37s 4ms/step - loss: 0.0854 - acc: 0.9749 - val_loss: 0.4822 - val_acc: 0.8816\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0917 - acc: 0.9733 - val_loss: 0.4816 - val_acc: 0.8802\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0914 - acc: 0.9724 - val_loss: 0.4869 - val_acc: 0.8758\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0914 - acc: 0.9717 - val_loss: 0.4895 - val_acc: 0.8794\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0878 - acc: 0.9732 - val_loss: 0.4875 - val_acc: 0.8785\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0827 - acc: 0.9777 - val_loss: 0.4905 - val_acc: 0.8785\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0886 - acc: 0.9744 - val_loss: 0.4901 - val_acc: 0.8772\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 43s 5ms/step - loss: 0.0845 - acc: 0.9764 - val_loss: 0.4880 - val_acc: 0.8789\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0882 - acc: 0.9726 - val_loss: 0.4910 - val_acc: 0.8776\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0860 - acc: 0.9739 - val_loss: 0.4940 - val_acc: 0.8745\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0885 - acc: 0.9754 - val_loss: 0.4914 - val_acc: 0.8749\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0833 - acc: 0.9734 - val_loss: 0.4920 - val_acc: 0.8767\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0861 - acc: 0.9756 - val_loss: 0.4930 - val_acc: 0.8794\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0781 - acc: 0.9765 - val_loss: 0.4928 - val_acc: 0.8776\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0829 - acc: 0.9748 - val_loss: 0.4921 - val_acc: 0.8767\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0829 - acc: 0.9736 - val_loss: 0.4948 - val_acc: 0.8798\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 39s 4ms/step - loss: 0.0815 - acc: 0.9753 - val_loss: 0.4942 - val_acc: 0.8745\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 38s 4ms/step - loss: 0.0901 - acc: 0.9743 - val_loss: 0.4978 - val_acc: 0.8741\n"
     ]
    }
   ],
   "source": [
    "hs_4_6 = model_i.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_i.save(\"Text_CNN_option_4_3rd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second update --- increase the number of filters for each filter size, from 64 to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 3.3801 - acc: 0.1991 - val_loss: 1.8754 - val_acc: 0.5577\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 1.6363 - acc: 0.4828 - val_loss: 1.4287 - val_acc: 0.6955\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 1.1910 - acc: 0.6260 - val_loss: 1.1573 - val_acc: 0.7662\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.9951 - acc: 0.6893 - val_loss: 1.0253 - val_acc: 0.7852\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.8362 - acc: 0.7427 - val_loss: 0.9251 - val_acc: 0.7976\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.7345 - acc: 0.7731 - val_loss: 0.8527 - val_acc: 0.8219\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.6619 - acc: 0.7934 - val_loss: 0.7952 - val_acc: 0.8263\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.5952 - acc: 0.8156 - val_loss: 0.7323 - val_acc: 0.8445\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.5627 - acc: 0.8257 - val_loss: 0.7000 - val_acc: 0.8431\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.5023 - acc: 0.8485 - val_loss: 0.6697 - val_acc: 0.8467\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.4786 - acc: 0.8526 - val_loss: 0.6327 - val_acc: 0.8520\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.4421 - acc: 0.8655 - val_loss: 0.6062 - val_acc: 0.8542\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.4113 - acc: 0.8742 - val_loss: 0.5902 - val_acc: 0.8573\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.3859 - acc: 0.8810 - val_loss: 0.5768 - val_acc: 0.8582\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.3619 - acc: 0.8929 - val_loss: 0.5469 - val_acc: 0.8630\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.3363 - acc: 0.8981 - val_loss: 0.5390 - val_acc: 0.8599\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.3245 - acc: 0.9017 - val_loss: 0.5147 - val_acc: 0.8670\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.3126 - acc: 0.9037 - val_loss: 0.5102 - val_acc: 0.8683\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2957 - acc: 0.9104 - val_loss: 0.4916 - val_acc: 0.8719\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2818 - acc: 0.9138 - val_loss: 0.4904 - val_acc: 0.8670\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2638 - acc: 0.9205 - val_loss: 0.4930 - val_acc: 0.8674\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2550 - acc: 0.9237 - val_loss: 0.4754 - val_acc: 0.8670\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.2419 - acc: 0.9251 - val_loss: 0.4692 - val_acc: 0.8723\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.2408 - acc: 0.9256 - val_loss: 0.4741 - val_acc: 0.8670\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.2166 - acc: 0.9369 - val_loss: 0.4681 - val_acc: 0.8683\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.2158 - acc: 0.9348 - val_loss: 0.4524 - val_acc: 0.8732\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.2203 - acc: 0.9342 - val_loss: 0.4610 - val_acc: 0.8714\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.2093 - acc: 0.9353 - val_loss: 0.4570 - val_acc: 0.8683\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1996 - acc: 0.9399 - val_loss: 0.4515 - val_acc: 0.8701\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1909 - acc: 0.9400 - val_loss: 0.4487 - val_acc: 0.8732\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1948 - acc: 0.9398 - val_loss: 0.4402 - val_acc: 0.8736\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1877 - acc: 0.9417 - val_loss: 0.4448 - val_acc: 0.8710\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1802 - acc: 0.9417 - val_loss: 0.4470 - val_acc: 0.8701\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1736 - acc: 0.9465 - val_loss: 0.4322 - val_acc: 0.8736\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1699 - acc: 0.9475 - val_loss: 0.4440 - val_acc: 0.8732\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1546 - acc: 0.9546 - val_loss: 0.4257 - val_acc: 0.8723\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1672 - acc: 0.9475 - val_loss: 0.4329 - val_acc: 0.8754\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1644 - acc: 0.9487 - val_loss: 0.4392 - val_acc: 0.8714\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1582 - acc: 0.9537 - val_loss: 0.4276 - val_acc: 0.8736\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.1556 - acc: 0.9519 - val_loss: 0.4341 - val_acc: 0.8732\n"
     ]
    }
   ],
   "source": [
    "in_x = Input(shape = (X_train.shape[1],X_train.shape[2]),dtype = 'float32')\n",
    "\n",
    "input_x = GaussianNoise(stddev = 0.01)(in_x)\n",
    "\n",
    "x1 = Conv1D(128,3,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "x2 = Conv1D(128,5,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "x3 = Conv1D(128,7,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "\n",
    "x1 = GlobalMaxPooling1D()(x1)# 128 dim vector\n",
    "x2 = GlobalMaxPooling1D()(x2) # 128 dim vector\n",
    "x3 = GlobalMaxPooling1D()(x3) # 128 dim vector\n",
    "\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2,x3])\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='softmax')(x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "\n",
    "model_5 = Model(in_x,preds)\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_5.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "hs_5_1 = model_5.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1509 - acc: 0.9546 - val_loss: 0.4233 - val_acc: 0.8736\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1474 - acc: 0.9535 - val_loss: 0.4225 - val_acc: 0.8772\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1562 - acc: 0.9532 - val_loss: 0.4213 - val_acc: 0.8767\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1528 - acc: 0.9514 - val_loss: 0.4250 - val_acc: 0.8723\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1381 - acc: 0.9567 - val_loss: 0.4281 - val_acc: 0.8723\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1370 - acc: 0.9572 - val_loss: 0.4339 - val_acc: 0.8789\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1444 - acc: 0.9562 - val_loss: 0.4336 - val_acc: 0.8749\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1313 - acc: 0.9603 - val_loss: 0.4412 - val_acc: 0.8776\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1406 - acc: 0.9587 - val_loss: 0.4245 - val_acc: 0.8763\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1279 - acc: 0.9613 - val_loss: 0.4349 - val_acc: 0.8745\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1278 - acc: 0.9631 - val_loss: 0.4254 - val_acc: 0.8736\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1186 - acc: 0.9617 - val_loss: 0.4397 - val_acc: 0.8719\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1150 - acc: 0.9666 - val_loss: 0.4327 - val_acc: 0.8745\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1234 - acc: 0.9634 - val_loss: 0.4386 - val_acc: 0.8741\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1245 - acc: 0.9631 - val_loss: 0.4238 - val_acc: 0.8763\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1208 - acc: 0.9620 - val_loss: 0.4257 - val_acc: 0.8802\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1154 - acc: 0.9632 - val_loss: 0.4217 - val_acc: 0.8798\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1094 - acc: 0.9660 - val_loss: 0.4265 - val_acc: 0.8798\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1039 - acc: 0.9677 - val_loss: 0.4239 - val_acc: 0.8776\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1209 - acc: 0.9621 - val_loss: 0.4231 - val_acc: 0.8807\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1181 - acc: 0.9644 - val_loss: 0.4218 - val_acc: 0.8798\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1103 - acc: 0.9671 - val_loss: 0.4205 - val_acc: 0.8772\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1093 - acc: 0.9673 - val_loss: 0.4372 - val_acc: 0.8772\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1149 - acc: 0.9641 - val_loss: 0.4402 - val_acc: 0.8789\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1082 - acc: 0.9666 - val_loss: 0.4346 - val_acc: 0.8802\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1026 - acc: 0.9674 - val_loss: 0.4299 - val_acc: 0.8736\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1011 - acc: 0.9686 - val_loss: 0.4365 - val_acc: 0.8785\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1040 - acc: 0.9665 - val_loss: 0.4309 - val_acc: 0.8767\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1068 - acc: 0.9675 - val_loss: 0.4334 - val_acc: 0.8798\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1107 - acc: 0.9681 - val_loss: 0.4398 - val_acc: 0.8833\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1037 - acc: 0.9669 - val_loss: 0.4346 - val_acc: 0.8825\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0915 - acc: 0.9720 - val_loss: 0.4302 - val_acc: 0.8833\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1002 - acc: 0.9685 - val_loss: 0.4362 - val_acc: 0.8772\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1013 - acc: 0.9684 - val_loss: 0.4437 - val_acc: 0.8705\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0893 - acc: 0.9729 - val_loss: 0.4496 - val_acc: 0.8776\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0988 - acc: 0.9716 - val_loss: 0.4598 - val_acc: 0.8719\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0969 - acc: 0.9707 - val_loss: 0.4491 - val_acc: 0.8794\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0986 - acc: 0.9709 - val_loss: 0.4593 - val_acc: 0.8741\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1016 - acc: 0.9695 - val_loss: 0.4454 - val_acc: 0.8763\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0930 - acc: 0.9718 - val_loss: 0.4669 - val_acc: 0.8723\n"
     ]
    }
   ],
   "source": [
    "hs_5_2 = model_5.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.save('TextCNN_option_5_1st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0797 - acc: 0.9750 - val_loss: 0.4459 - val_acc: 0.8785\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0783 - acc: 0.9778 - val_loss: 0.4397 - val_acc: 0.8820\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0715 - acc: 0.9782 - val_loss: 0.4376 - val_acc: 0.8833\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0775 - acc: 0.9756 - val_loss: 0.4386 - val_acc: 0.8833\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.0697 - acc: 0.9788 - val_loss: 0.4365 - val_acc: 0.8856\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0658 - acc: 0.9802 - val_loss: 0.4451 - val_acc: 0.8851\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0652 - acc: 0.9806 - val_loss: 0.4440 - val_acc: 0.8829\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0666 - acc: 0.9808 - val_loss: 0.4465 - val_acc: 0.8798\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0705 - acc: 0.9790 - val_loss: 0.4456 - val_acc: 0.8798\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0610 - acc: 0.9825 - val_loss: 0.4480 - val_acc: 0.8816\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0635 - acc: 0.9807 - val_loss: 0.4494 - val_acc: 0.8816\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0566 - acc: 0.9824 - val_loss: 0.4582 - val_acc: 0.8811\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.0590 - acc: 0.9821 - val_loss: 0.4479 - val_acc: 0.8856\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0568 - acc: 0.9822 - val_loss: 0.4410 - val_acc: 0.8833\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0497 - acc: 0.9845 - val_loss: 0.4540 - val_acc: 0.8807\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0615 - acc: 0.9811 - val_loss: 0.4423 - val_acc: 0.8838\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0551 - acc: 0.9840 - val_loss: 0.4473 - val_acc: 0.8807\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0597 - acc: 0.9825 - val_loss: 0.4373 - val_acc: 0.8789\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0544 - acc: 0.9845 - val_loss: 0.4371 - val_acc: 0.8847\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0549 - acc: 0.9840 - val_loss: 0.4361 - val_acc: 0.8825\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.0555 - acc: 0.9846 - val_loss: 0.4507 - val_acc: 0.8825\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0578 - acc: 0.9833 - val_loss: 0.4486 - val_acc: 0.8802\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0558 - acc: 0.9838 - val_loss: 0.4455 - val_acc: 0.8864\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0560 - acc: 0.9851 - val_loss: 0.4475 - val_acc: 0.8825\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0540 - acc: 0.9835 - val_loss: 0.4443 - val_acc: 0.8864\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0506 - acc: 0.9840 - val_loss: 0.4431 - val_acc: 0.8798\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0558 - acc: 0.9838 - val_loss: 0.4442 - val_acc: 0.8820\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0513 - acc: 0.9859 - val_loss: 0.4483 - val_acc: 0.8833\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0560 - acc: 0.9841 - val_loss: 0.4468 - val_acc: 0.8802\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0480 - acc: 0.9857 - val_loss: 0.4524 - val_acc: 0.8816\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0509 - acc: 0.9860 - val_loss: 0.4450 - val_acc: 0.8833\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0492 - acc: 0.9857 - val_loss: 0.4500 - val_acc: 0.8860\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0471 - acc: 0.9860 - val_loss: 0.4495 - val_acc: 0.8802\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0488 - acc: 0.9864 - val_loss: 0.4466 - val_acc: 0.8847\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0450 - acc: 0.9852 - val_loss: 0.4406 - val_acc: 0.8851\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0504 - acc: 0.9842 - val_loss: 0.4497 - val_acc: 0.8811\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0425 - acc: 0.9872 - val_loss: 0.4476 - val_acc: 0.8838\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0455 - acc: 0.9870 - val_loss: 0.4502 - val_acc: 0.8829\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0492 - acc: 0.9855 - val_loss: 0.4547 - val_acc: 0.8838\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0428 - acc: 0.9866 - val_loss: 0.4488 - val_acc: 0.8900\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_5.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "hs_5_3 = model_5.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.save('TextCNN_option_5_2nd')#val_acc:8900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0436 - acc: 0.9878 - val_loss: 0.4541 - val_acc: 0.8851\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0544 - acc: 0.9856 - val_loss: 0.4446 - val_acc: 0.8851\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.0474 - acc: 0.9877 - val_loss: 0.4521 - val_acc: 0.8838\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0481 - acc: 0.9861 - val_loss: 0.4537 - val_acc: 0.8882\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0454 - acc: 0.9863 - val_loss: 0.4517 - val_acc: 0.8829\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0411 - acc: 0.9874 - val_loss: 0.4562 - val_acc: 0.8838\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.0455 - acc: 0.9865 - val_loss: 0.4641 - val_acc: 0.8851\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.0438 - acc: 0.9865 - val_loss: 0.4508 - val_acc: 0.8873\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0486 - acc: 0.9863 - val_loss: 0.4558 - val_acc: 0.8860\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0462 - acc: 0.9853 - val_loss: 0.4524 - val_acc: 0.8895\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0425 - acc: 0.9881 - val_loss: 0.4531 - val_acc: 0.8842\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0420 - acc: 0.9873 - val_loss: 0.4599 - val_acc: 0.8820\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0415 - acc: 0.9870 - val_loss: 0.4584 - val_acc: 0.8882\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.0438 - acc: 0.9871 - val_loss: 0.4683 - val_acc: 0.8798\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0444 - acc: 0.9872 - val_loss: 0.4643 - val_acc: 0.8829\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0438 - acc: 0.9869 - val_loss: 0.4554 - val_acc: 0.8869\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0370 - acc: 0.9887 - val_loss: 0.4565 - val_acc: 0.8851\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0411 - acc: 0.9866 - val_loss: 0.4599 - val_acc: 0.8864\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0381 - acc: 0.9880 - val_loss: 0.4762 - val_acc: 0.8842\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0436 - acc: 0.9874 - val_loss: 0.4761 - val_acc: 0.8833\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0425 - acc: 0.9870 - val_loss: 0.4587 - val_acc: 0.8825\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0451 - acc: 0.9860 - val_loss: 0.4537 - val_acc: 0.8891\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0447 - acc: 0.9864 - val_loss: 0.4663 - val_acc: 0.8838\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0375 - acc: 0.9872 - val_loss: 0.4648 - val_acc: 0.8847\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0349 - acc: 0.9901 - val_loss: 0.4749 - val_acc: 0.8856\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0395 - acc: 0.9877 - val_loss: 0.4589 - val_acc: 0.8856\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0362 - acc: 0.9895 - val_loss: 0.4553 - val_acc: 0.8851\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0380 - acc: 0.9897 - val_loss: 0.4643 - val_acc: 0.8829\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0437 - acc: 0.9890 - val_loss: 0.4697 - val_acc: 0.8825\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.0376 - acc: 0.9894 - val_loss: 0.4654 - val_acc: 0.8825\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.0397 - acc: 0.9877 - val_loss: 0.4662 - val_acc: 0.8816\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0392 - acc: 0.9888 - val_loss: 0.4649 - val_acc: 0.8838\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0416 - acc: 0.9881 - val_loss: 0.4734 - val_acc: 0.8789\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0354 - acc: 0.9896 - val_loss: 0.4720 - val_acc: 0.8825\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0373 - acc: 0.9886 - val_loss: 0.4698 - val_acc: 0.8856\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.0353 - acc: 0.9895 - val_loss: 0.4672 - val_acc: 0.8856\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0389 - acc: 0.9894 - val_loss: 0.4663 - val_acc: 0.8886\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.0387 - acc: 0.9880 - val_loss: 0.4716 - val_acc: 0.8856\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.0336 - acc: 0.9896 - val_loss: 0.4678 - val_acc: 0.8873\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.0370 - acc: 0.9890 - val_loss: 0.4759 - val_acc: 0.8838\n"
     ]
    }
   ],
   "source": [
    "hs_5_4 = model_5.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.save('TextCNN_option_5_3rd')#val_acc:8900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0394 - acc: 0.9884 - val_loss: 0.4703 - val_acc: 0.8829\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0414 - acc: 0.9873 - val_loss: 0.4754 - val_acc: 0.8838\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0339 - acc: 0.9905 - val_loss: 0.4810 - val_acc: 0.8838\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0387 - acc: 0.9895 - val_loss: 0.4803 - val_acc: 0.8838\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.0405 - acc: 0.9883 - val_loss: 0.4736 - val_acc: 0.8895\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0351 - acc: 0.9886 - val_loss: 0.4867 - val_acc: 0.8833\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0354 - acc: 0.9907 - val_loss: 0.4809 - val_acc: 0.8833\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0366 - acc: 0.9902 - val_loss: 0.4836 - val_acc: 0.8847\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0379 - acc: 0.9885 - val_loss: 0.4722 - val_acc: 0.8878\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0410 - acc: 0.9895 - val_loss: 0.4894 - val_acc: 0.8816\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0444 - acc: 0.9866 - val_loss: 0.4851 - val_acc: 0.8829\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0371 - acc: 0.9902 - val_loss: 0.4840 - val_acc: 0.8864\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0353 - acc: 0.9905 - val_loss: 0.4792 - val_acc: 0.8882\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0359 - acc: 0.9890 - val_loss: 0.4792 - val_acc: 0.8882\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0356 - acc: 0.9901 - val_loss: 0.4728 - val_acc: 0.8935\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0381 - acc: 0.9886 - val_loss: 0.4616 - val_acc: 0.8900\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0369 - acc: 0.9905 - val_loss: 0.4686 - val_acc: 0.8864\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0374 - acc: 0.9886 - val_loss: 0.4685 - val_acc: 0.8878\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0314 - acc: 0.9907 - val_loss: 0.4729 - val_acc: 0.8869\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.0357 - acc: 0.9903 - val_loss: 0.4784 - val_acc: 0.8882\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0371 - acc: 0.9892 - val_loss: 0.4704 - val_acc: 0.8886\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0377 - acc: 0.9905 - val_loss: 0.4753 - val_acc: 0.8864\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0365 - acc: 0.9895 - val_loss: 0.4761 - val_acc: 0.8869\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0404 - acc: 0.9895 - val_loss: 0.4777 - val_acc: 0.8829\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0388 - acc: 0.9892 - val_loss: 0.4792 - val_acc: 0.8856\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0375 - acc: 0.9891 - val_loss: 0.4875 - val_acc: 0.8838\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0373 - acc: 0.9901 - val_loss: 0.4690 - val_acc: 0.8869\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0355 - acc: 0.9893 - val_loss: 0.4716 - val_acc: 0.8851\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.0343 - acc: 0.9905 - val_loss: 0.4765 - val_acc: 0.8851\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0372 - acc: 0.9903 - val_loss: 0.4754 - val_acc: 0.8873\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0319 - acc: 0.9913 - val_loss: 0.4754 - val_acc: 0.8864\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0288 - acc: 0.9915 - val_loss: 0.4861 - val_acc: 0.8838\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0297 - acc: 0.9915 - val_loss: 0.4777 - val_acc: 0.8856\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.0342 - acc: 0.9892 - val_loss: 0.4808 - val_acc: 0.8829\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.0397 - acc: 0.9891 - val_loss: 0.4932 - val_acc: 0.8802\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0360 - acc: 0.9905 - val_loss: 0.4810 - val_acc: 0.8842\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.0353 - acc: 0.9894 - val_loss: 0.4744 - val_acc: 0.8891\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0330 - acc: 0.9888 - val_loss: 0.4806 - val_acc: 0.8807\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.0385 - acc: 0.9884 - val_loss: 0.4806 - val_acc: 0.8838\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0308 - acc: 0.9912 - val_loss: 0.4821 - val_acc: 0.8860\n"
     ]
    }
   ],
   "source": [
    "hs_5_5 = model_5.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#8935"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third update --- introduce kernel_regularizer to the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 3.6893 - acc: 0.2076 - val_loss: 2.2028 - val_acc: 0.5833\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 1.9220 - acc: 0.4923 - val_loss: 1.7423 - val_acc: 0.7066\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 1.4795 - acc: 0.6187 - val_loss: 1.4940 - val_acc: 0.7662\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 1.2665 - acc: 0.6860 - val_loss: 1.3584 - val_acc: 0.7879\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 1.1032 - acc: 0.7359 - val_loss: 1.2533 - val_acc: 0.8122\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 1.0050 - acc: 0.7617 - val_loss: 1.1765 - val_acc: 0.8113\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.9223 - acc: 0.7860 - val_loss: 1.1207 - val_acc: 0.8219\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.8683 - acc: 0.7973 - val_loss: 1.0675 - val_acc: 0.8422\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.8104 - acc: 0.8205 - val_loss: 1.0291 - val_acc: 0.8374\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.7618 - acc: 0.8323 - val_loss: 1.0083 - val_acc: 0.8316\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.7332 - acc: 0.8402 - val_loss: 0.9689 - val_acc: 0.8480\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.7053 - acc: 0.8457 - val_loss: 0.9415 - val_acc: 0.8498\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.6629 - acc: 0.8579 - val_loss: 0.9173 - val_acc: 0.8529\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.6413 - acc: 0.8674 - val_loss: 0.9033 - val_acc: 0.8498\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.6295 - acc: 0.8714 - val_loss: 0.8783 - val_acc: 0.8608\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.6020 - acc: 0.8770 - val_loss: 0.8619 - val_acc: 0.8604\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.5897 - acc: 0.8837 - val_loss: 0.8478 - val_acc: 0.8696\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.5634 - acc: 0.8897 - val_loss: 0.8285 - val_acc: 0.8679\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.5595 - acc: 0.8898 - val_loss: 0.8339 - val_acc: 0.8630\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.5426 - acc: 0.8908 - val_loss: 0.8164 - val_acc: 0.8612\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.5166 - acc: 0.8976 - val_loss: 0.7983 - val_acc: 0.8688\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.5111 - acc: 0.9012 - val_loss: 0.7923 - val_acc: 0.8657\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.5000 - acc: 0.9068 - val_loss: 0.8092 - val_acc: 0.8665\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.4818 - acc: 0.9127 - val_loss: 0.7658 - val_acc: 0.8741\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.4795 - acc: 0.9126 - val_loss: 0.7692 - val_acc: 0.8639\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.4678 - acc: 0.9168 - val_loss: 0.7533 - val_acc: 0.8732\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.4595 - acc: 0.9170 - val_loss: 0.7426 - val_acc: 0.8741\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.4547 - acc: 0.9178 - val_loss: 0.7405 - val_acc: 0.8665\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.4437 - acc: 0.9249 - val_loss: 0.7392 - val_acc: 0.8763\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.4408 - acc: 0.9205 - val_loss: 0.7200 - val_acc: 0.8754\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.4237 - acc: 0.9246 - val_loss: 0.7178 - val_acc: 0.8727\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.4204 - acc: 0.9275 - val_loss: 0.7132 - val_acc: 0.8727\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.4107 - acc: 0.9304 - val_loss: 0.7087 - val_acc: 0.8802\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.4045 - acc: 0.9315 - val_loss: 0.7166 - val_acc: 0.8719\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.4030 - acc: 0.9334 - val_loss: 0.6977 - val_acc: 0.8745\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.3846 - acc: 0.9359 - val_loss: 0.6933 - val_acc: 0.8754\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.3819 - acc: 0.9397 - val_loss: 0.6972 - val_acc: 0.8785\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.3769 - acc: 0.9389 - val_loss: 0.6776 - val_acc: 0.8842\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.3744 - acc: 0.9383 - val_loss: 0.6815 - val_acc: 0.8794\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.3697 - acc: 0.9387 - val_loss: 0.6995 - val_acc: 0.8652\n"
     ]
    }
   ],
   "source": [
    "in_x = Input(shape = (X_train.shape[1],X_train.shape[2]),dtype = 'float32')\n",
    "\n",
    "input_x = GaussianNoise(stddev = 0.01)(in_x)\n",
    "\n",
    "x1 = Conv1D(128,3,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "x2 = Conv1D(128,5,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "x3 = Conv1D(128,7,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "\n",
    "x1 = GlobalMaxPooling1D()(x1)# 128 dim vector\n",
    "x2 = GlobalMaxPooling1D()(x2) # 128 dim vector\n",
    "x3 = GlobalMaxPooling1D()(x3) # 128 dim vector\n",
    "\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2,x3])\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='softmax',kernel_regularizer = regularizers.l2(0.01))(x)#\n",
    "\n",
    "model_8 = Model(in_x,preds)\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_8.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "hs_8_1 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.3676 - acc: 0.9366 - val_loss: 0.6831 - val_acc: 0.8763\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.3557 - acc: 0.9459 - val_loss: 0.6633 - val_acc: 0.8816\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.3506 - acc: 0.9435 - val_loss: 0.6693 - val_acc: 0.8794\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.3468 - acc: 0.9464 - val_loss: 0.6624 - val_acc: 0.8776\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.3497 - acc: 0.9428 - val_loss: 0.6612 - val_acc: 0.8825\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.3344 - acc: 0.9487 - val_loss: 0.6572 - val_acc: 0.8776\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.3373 - acc: 0.9462 - val_loss: 0.6525 - val_acc: 0.8825\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.3278 - acc: 0.9482 - val_loss: 0.6471 - val_acc: 0.8847\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.3265 - acc: 0.9495 - val_loss: 0.6449 - val_acc: 0.8789\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.3188 - acc: 0.9519 - val_loss: 0.6339 - val_acc: 0.8847\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.3176 - acc: 0.9520 - val_loss: 0.6337 - val_acc: 0.8816\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.3135 - acc: 0.9532 - val_loss: 0.6398 - val_acc: 0.8838\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.3081 - acc: 0.9528 - val_loss: 0.6361 - val_acc: 0.8794\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.3082 - acc: 0.9548 - val_loss: 0.6393 - val_acc: 0.8767\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.3128 - acc: 0.9505 - val_loss: 0.6223 - val_acc: 0.8882\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2972 - acc: 0.9569 - val_loss: 0.6188 - val_acc: 0.8847\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.3024 - acc: 0.9526 - val_loss: 0.6178 - val_acc: 0.8847\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 77s 8ms/step - loss: 0.3014 - acc: 0.9553 - val_loss: 0.6105 - val_acc: 0.8842\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2922 - acc: 0.9578 - val_loss: 0.6101 - val_acc: 0.8860\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2977 - acc: 0.9525 - val_loss: 0.6143 - val_acc: 0.8882\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.2890 - acc: 0.9581 - val_loss: 0.6155 - val_acc: 0.8829\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2820 - acc: 0.9604 - val_loss: 0.6093 - val_acc: 0.8825\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2855 - acc: 0.9580 - val_loss: 0.6181 - val_acc: 0.8772\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2785 - acc: 0.9598 - val_loss: 0.6050 - val_acc: 0.8851\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2797 - acc: 0.9576 - val_loss: 0.6062 - val_acc: 0.8807\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2757 - acc: 0.9613 - val_loss: 0.6036 - val_acc: 0.8802\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2679 - acc: 0.9629 - val_loss: 0.5954 - val_acc: 0.8825\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2817 - acc: 0.9568 - val_loss: 0.6040 - val_acc: 0.8829\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2739 - acc: 0.9590 - val_loss: 0.6056 - val_acc: 0.8869\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2662 - acc: 0.9632 - val_loss: 0.5897 - val_acc: 0.8869\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2614 - acc: 0.9631 - val_loss: 0.5885 - val_acc: 0.8842\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2669 - acc: 0.9621 - val_loss: 0.5829 - val_acc: 0.8895\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2665 - acc: 0.9614 - val_loss: 0.5843 - val_acc: 0.8864\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.2556 - acc: 0.9643 - val_loss: 0.5836 - val_acc: 0.8869\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2576 - acc: 0.9650 - val_loss: 0.5885 - val_acc: 0.8913\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.2525 - acc: 0.9669 - val_loss: 0.5890 - val_acc: 0.8838\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.2508 - acc: 0.9655 - val_loss: 0.5843 - val_acc: 0.8860\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2585 - acc: 0.9637 - val_loss: 0.5883 - val_acc: 0.8816\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2504 - acc: 0.9638 - val_loss: 0.5832 - val_acc: 0.8878\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.2456 - acc: 0.9670 - val_loss: 0.5890 - val_acc: 0.8825\n"
     ]
    }
   ],
   "source": [
    "hs_8_2 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#8913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_2nd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.2495 - acc: 0.9663 - val_loss: 0.5836 - val_acc: 0.8825\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2498 - acc: 0.9630 - val_loss: 0.5859 - val_acc: 0.8811\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.2378 - acc: 0.9674 - val_loss: 0.5846 - val_acc: 0.8816\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.2366 - acc: 0.9706 - val_loss: 0.5665 - val_acc: 0.8926\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2288 - acc: 0.9694 - val_loss: 0.5693 - val_acc: 0.8833\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2396 - acc: 0.9664 - val_loss: 0.5594 - val_acc: 0.8886\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.2368 - acc: 0.9677 - val_loss: 0.5618 - val_acc: 0.8913\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2283 - acc: 0.9687 - val_loss: 0.5624 - val_acc: 0.8900\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2293 - acc: 0.9696 - val_loss: 0.5612 - val_acc: 0.8842\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2263 - acc: 0.9715 - val_loss: 0.5600 - val_acc: 0.8909\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.2309 - acc: 0.9687 - val_loss: 0.5689 - val_acc: 0.8838\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2280 - acc: 0.9697 - val_loss: 0.5606 - val_acc: 0.8825\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.2286 - acc: 0.9675 - val_loss: 0.5542 - val_acc: 0.8895\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2251 - acc: 0.9708 - val_loss: 0.5594 - val_acc: 0.8816\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2178 - acc: 0.9730 - val_loss: 0.5694 - val_acc: 0.8842\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2239 - acc: 0.9696 - val_loss: 0.5580 - val_acc: 0.8860\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2156 - acc: 0.9740 - val_loss: 0.5513 - val_acc: 0.8878\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2135 - acc: 0.9719 - val_loss: 0.5438 - val_acc: 0.8895\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2140 - acc: 0.9692 - val_loss: 0.5484 - val_acc: 0.8909\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.2188 - acc: 0.9703 - val_loss: 0.5522 - val_acc: 0.8882\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.2190 - acc: 0.9707 - val_loss: 0.5483 - val_acc: 0.8878\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2180 - acc: 0.9702 - val_loss: 0.5619 - val_acc: 0.8842\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2118 - acc: 0.9723 - val_loss: 0.5522 - val_acc: 0.8882\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2141 - acc: 0.9717 - val_loss: 0.5429 - val_acc: 0.8922\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2097 - acc: 0.9708 - val_loss: 0.5486 - val_acc: 0.8873\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2147 - acc: 0.9702 - val_loss: 0.5429 - val_acc: 0.8922\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2026 - acc: 0.9754 - val_loss: 0.5517 - val_acc: 0.8900\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.2096 - acc: 0.9722 - val_loss: 0.5481 - val_acc: 0.8856\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2097 - acc: 0.9711 - val_loss: 0.5398 - val_acc: 0.8895\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2066 - acc: 0.9728 - val_loss: 0.5420 - val_acc: 0.8833\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2009 - acc: 0.9740 - val_loss: 0.5291 - val_acc: 0.8882\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2049 - acc: 0.9741 - val_loss: 0.5286 - val_acc: 0.8926\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1980 - acc: 0.9759 - val_loss: 0.5210 - val_acc: 0.8922\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2009 - acc: 0.9718 - val_loss: 0.5315 - val_acc: 0.8926\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1970 - acc: 0.9753 - val_loss: 0.5363 - val_acc: 0.8882\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2031 - acc: 0.9727 - val_loss: 0.5402 - val_acc: 0.8860\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.2036 - acc: 0.9733 - val_loss: 0.5364 - val_acc: 0.8842\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1981 - acc: 0.9747 - val_loss: 0.5249 - val_acc: 0.8926\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.2003 - acc: 0.9736 - val_loss: 0.5366 - val_acc: 0.8882\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1993 - acc: 0.9722 - val_loss: 0.5448 - val_acc: 0.8895\n"
     ]
    }
   ],
   "source": [
    "hs_8_3 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#8926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_3rd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1879 - acc: 0.9777 - val_loss: 0.5289 - val_acc: 0.8917\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1951 - acc: 0.9741 - val_loss: 0.5353 - val_acc: 0.8895\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1877 - acc: 0.9776 - val_loss: 0.5398 - val_acc: 0.8820\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 66s 7ms/step - loss: 0.1977 - acc: 0.9746 - val_loss: 0.5269 - val_acc: 0.8904\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1927 - acc: 0.9764 - val_loss: 0.5457 - val_acc: 0.8780\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 66s 7ms/step - loss: 0.1912 - acc: 0.9751 - val_loss: 0.5217 - val_acc: 0.8900\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1902 - acc: 0.9744 - val_loss: 0.5236 - val_acc: 0.8922\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1894 - acc: 0.9762 - val_loss: 0.5221 - val_acc: 0.8904\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1886 - acc: 0.9726 - val_loss: 0.5278 - val_acc: 0.8882\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1851 - acc: 0.9757 - val_loss: 0.5227 - val_acc: 0.8847\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1908 - acc: 0.9736 - val_loss: 0.5177 - val_acc: 0.8900\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1868 - acc: 0.9751 - val_loss: 0.5318 - val_acc: 0.8851\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1938 - acc: 0.9732 - val_loss: 0.5216 - val_acc: 0.8891\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1839 - acc: 0.9776 - val_loss: 0.5134 - val_acc: 0.8935\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1864 - acc: 0.9744 - val_loss: 0.5191 - val_acc: 0.8948\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1806 - acc: 0.9765 - val_loss: 0.5243 - val_acc: 0.8886\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1825 - acc: 0.9783 - val_loss: 0.5141 - val_acc: 0.8922\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1810 - acc: 0.9761 - val_loss: 0.5132 - val_acc: 0.8864\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1829 - acc: 0.9779 - val_loss: 0.5092 - val_acc: 0.8904\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1759 - acc: 0.9778 - val_loss: 0.5135 - val_acc: 0.8891\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1827 - acc: 0.9750 - val_loss: 0.5202 - val_acc: 0.8825\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1783 - acc: 0.9778 - val_loss: 0.5067 - val_acc: 0.8913\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1806 - acc: 0.9757 - val_loss: 0.5130 - val_acc: 0.8869\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1784 - acc: 0.9772 - val_loss: 0.5212 - val_acc: 0.8807\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1740 - acc: 0.9794 - val_loss: 0.5032 - val_acc: 0.8895\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1725 - acc: 0.9783 - val_loss: 0.5234 - val_acc: 0.8900\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1738 - acc: 0.9790 - val_loss: 0.5101 - val_acc: 0.8864\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1718 - acc: 0.9788 - val_loss: 0.5046 - val_acc: 0.8917\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1763 - acc: 0.9775 - val_loss: 0.5035 - val_acc: 0.8909\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1768 - acc: 0.9764 - val_loss: 0.5050 - val_acc: 0.8922\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1714 - acc: 0.9786 - val_loss: 0.5082 - val_acc: 0.8895\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1719 - acc: 0.9764 - val_loss: 0.5134 - val_acc: 0.8842\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1686 - acc: 0.9794 - val_loss: 0.5055 - val_acc: 0.8939\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1716 - acc: 0.9771 - val_loss: 0.5001 - val_acc: 0.8979\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1749 - acc: 0.9758 - val_loss: 0.5083 - val_acc: 0.8891\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1665 - acc: 0.9806 - val_loss: 0.5025 - val_acc: 0.8886\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1678 - acc: 0.9771 - val_loss: 0.5155 - val_acc: 0.8864\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1705 - acc: 0.9776 - val_loss: 0.4946 - val_acc: 0.8926\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1686 - acc: 0.9796 - val_loss: 0.4959 - val_acc: 0.8926\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1647 - acc: 0.9817 - val_loss: 0.5136 - val_acc: 0.8873\n"
     ]
    }
   ],
   "source": [
    "hs_8_4 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#8979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_4th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1686 - acc: 0.9775 - val_loss: 0.5031 - val_acc: 0.8895\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1616 - acc: 0.9812 - val_loss: 0.5029 - val_acc: 0.8909\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1626 - acc: 0.9790 - val_loss: 0.5020 - val_acc: 0.8878\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1559 - acc: 0.9808 - val_loss: 0.5021 - val_acc: 0.8851\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.1700 - acc: 0.9759 - val_loss: 0.4895 - val_acc: 0.8953\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1652 - acc: 0.9787 - val_loss: 0.5039 - val_acc: 0.8891\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1573 - acc: 0.9812 - val_loss: 0.5048 - val_acc: 0.8913\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1659 - acc: 0.9779 - val_loss: 0.4985 - val_acc: 0.8882\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.1608 - acc: 0.9811 - val_loss: 0.5021 - val_acc: 0.8891\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1588 - acc: 0.9811 - val_loss: 0.5078 - val_acc: 0.8851\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1543 - acc: 0.9833 - val_loss: 0.5054 - val_acc: 0.8878\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1641 - acc: 0.9789 - val_loss: 0.5064 - val_acc: 0.8882\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1569 - acc: 0.9796 - val_loss: 0.5113 - val_acc: 0.8838\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1608 - acc: 0.9798 - val_loss: 0.4887 - val_acc: 0.8931\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.1547 - acc: 0.9811 - val_loss: 0.5112 - val_acc: 0.8833\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1580 - acc: 0.9794 - val_loss: 0.4948 - val_acc: 0.8891\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1560 - acc: 0.9811 - val_loss: 0.4875 - val_acc: 0.8939\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1586 - acc: 0.9817 - val_loss: 0.4824 - val_acc: 0.8922\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1580 - acc: 0.9798 - val_loss: 0.5057 - val_acc: 0.8856\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1544 - acc: 0.9812 - val_loss: 0.4826 - val_acc: 0.8926\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1581 - acc: 0.9791 - val_loss: 0.4863 - val_acc: 0.8909\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1559 - acc: 0.9792 - val_loss: 0.4798 - val_acc: 0.8939\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1555 - acc: 0.9797 - val_loss: 0.4944 - val_acc: 0.8939\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1582 - acc: 0.9797 - val_loss: 0.4940 - val_acc: 0.8900\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1580 - acc: 0.9787 - val_loss: 0.4928 - val_acc: 0.8904\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 98s 11ms/step - loss: 0.1559 - acc: 0.9798 - val_loss: 0.4955 - val_acc: 0.8895\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 120s 13ms/step - loss: 0.1516 - acc: 0.9827 - val_loss: 0.4862 - val_acc: 0.8944\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.1485 - acc: 0.9836 - val_loss: 0.5038 - val_acc: 0.8860\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 77s 9ms/step - loss: 0.1550 - acc: 0.9793 - val_loss: 0.4863 - val_acc: 0.8957\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1572 - acc: 0.9791 - val_loss: 0.4852 - val_acc: 0.8953\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1521 - acc: 0.9806 - val_loss: 0.5079 - val_acc: 0.8825\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1542 - acc: 0.9804 - val_loss: 0.4943 - val_acc: 0.8882\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1510 - acc: 0.9799 - val_loss: 0.4929 - val_acc: 0.8909\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1486 - acc: 0.9807 - val_loss: 0.4886 - val_acc: 0.8904\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1479 - acc: 0.9813 - val_loss: 0.4931 - val_acc: 0.8909\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 77s 8ms/step - loss: 0.1479 - acc: 0.9814 - val_loss: 0.4861 - val_acc: 0.8922\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1493 - acc: 0.9808 - val_loss: 0.4821 - val_acc: 0.8917\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1463 - acc: 0.9811 - val_loss: 0.5008 - val_acc: 0.8860\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1444 - acc: 0.9833 - val_loss: 0.4864 - val_acc: 0.8860\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1522 - acc: 0.9789 - val_loss: 0.4806 - val_acc: 0.8909\n"
     ]
    }
   ],
   "source": [
    "hs_8_5 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))#8979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_5th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8 = load_model(\"TextCNN_option_8_5th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1399 - acc: 0.9849 - val_loss: 0.4792 - val_acc: 0.8948\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1391 - acc: 0.9851 - val_loss: 0.4817 - val_acc: 0.8931\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1390 - acc: 0.9842 - val_loss: 0.4857 - val_acc: 0.8878\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1335 - acc: 0.9876 - val_loss: 0.4775 - val_acc: 0.8966\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1344 - acc: 0.9843 - val_loss: 0.4881 - val_acc: 0.8917\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1314 - acc: 0.9872 - val_loss: 0.4752 - val_acc: 0.8913\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1307 - acc: 0.9877 - val_loss: 0.4781 - val_acc: 0.8882\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1351 - acc: 0.9860 - val_loss: 0.4755 - val_acc: 0.8953\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1324 - acc: 0.9850 - val_loss: 0.4782 - val_acc: 0.8931\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1334 - acc: 0.9860 - val_loss: 0.4809 - val_acc: 0.8904\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1286 - acc: 0.9865 - val_loss: 0.4879 - val_acc: 0.8873\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1332 - acc: 0.9840 - val_loss: 0.4737 - val_acc: 0.8939\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1265 - acc: 0.9881 - val_loss: 0.4737 - val_acc: 0.8939\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1272 - acc: 0.9870 - val_loss: 0.4779 - val_acc: 0.8895\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1266 - acc: 0.9872 - val_loss: 0.4768 - val_acc: 0.8891\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1305 - acc: 0.9852 - val_loss: 0.4702 - val_acc: 0.8904\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1278 - acc: 0.9871 - val_loss: 0.4798 - val_acc: 0.8909\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1314 - acc: 0.9840 - val_loss: 0.4722 - val_acc: 0.8953\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1256 - acc: 0.9876 - val_loss: 0.4804 - val_acc: 0.8900\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1247 - acc: 0.9876 - val_loss: 0.4711 - val_acc: 0.8931\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1263 - acc: 0.9873 - val_loss: 0.4823 - val_acc: 0.8891\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1253 - acc: 0.9866 - val_loss: 0.4657 - val_acc: 0.8926\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1259 - acc: 0.9867 - val_loss: 0.4687 - val_acc: 0.8939\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1256 - acc: 0.9866 - val_loss: 0.4753 - val_acc: 0.8948\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1216 - acc: 0.9883 - val_loss: 0.4717 - val_acc: 0.8922\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1214 - acc: 0.9878 - val_loss: 0.4779 - val_acc: 0.8926\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1277 - acc: 0.9882 - val_loss: 0.4742 - val_acc: 0.8966\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1259 - acc: 0.9861 - val_loss: 0.4731 - val_acc: 0.8931\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1260 - acc: 0.9862 - val_loss: 0.4846 - val_acc: 0.8900\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1264 - acc: 0.9865 - val_loss: 0.4728 - val_acc: 0.8953\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1225 - acc: 0.9881 - val_loss: 0.4673 - val_acc: 0.8966\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1247 - acc: 0.9872 - val_loss: 0.4691 - val_acc: 0.8939\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1246 - acc: 0.9871 - val_loss: 0.4719 - val_acc: 0.8935\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1223 - acc: 0.9869 - val_loss: 0.4651 - val_acc: 0.8931\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1233 - acc: 0.9866 - val_loss: 0.4724 - val_acc: 0.8931\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1195 - acc: 0.9892 - val_loss: 0.4691 - val_acc: 0.8931\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1219 - acc: 0.9886 - val_loss: 0.4739 - val_acc: 0.8931\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1248 - acc: 0.9863 - val_loss: 0.4698 - val_acc: 0.8935\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1213 - acc: 0.9878 - val_loss: 0.4698 - val_acc: 0.8957\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1199 - acc: 0.9880 - val_loss: 0.4698 - val_acc: 0.8970\n"
     ]
    }
   ],
   "source": [
    "optimizer_8_6 = optimizers.RMSprop(lr = 0.0005)\n",
    "model_8.compile(loss = 'categorical_crossentropy',optimizer = optimizer_8_6,metrics = ['acc'])\n",
    "hs_8_6 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_6th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1225 - acc: 0.9876 - val_loss: 0.4660 - val_acc: 0.8926\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1203 - acc: 0.9874 - val_loss: 0.4659 - val_acc: 0.8975\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1195 - acc: 0.9875 - val_loss: 0.4627 - val_acc: 0.8975\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1242 - acc: 0.9870 - val_loss: 0.4711 - val_acc: 0.8917\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1184 - acc: 0.9882 - val_loss: 0.4759 - val_acc: 0.8926\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1214 - acc: 0.9871 - val_loss: 0.4667 - val_acc: 0.8966\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1177 - acc: 0.9885 - val_loss: 0.4751 - val_acc: 0.8909\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1170 - acc: 0.9899 - val_loss: 0.4714 - val_acc: 0.8922\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1212 - acc: 0.9867 - val_loss: 0.4703 - val_acc: 0.8975\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1207 - acc: 0.9867 - val_loss: 0.4748 - val_acc: 0.8957\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1214 - acc: 0.9883 - val_loss: 0.4696 - val_acc: 0.8917\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1205 - acc: 0.9883 - val_loss: 0.4728 - val_acc: 0.8904\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1184 - acc: 0.9875 - val_loss: 0.4625 - val_acc: 0.8953\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1179 - acc: 0.9877 - val_loss: 0.4682 - val_acc: 0.8948\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1250 - acc: 0.9860 - val_loss: 0.4766 - val_acc: 0.8917\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1172 - acc: 0.9877 - val_loss: 0.4634 - val_acc: 0.8935\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.1176 - acc: 0.9892 - val_loss: 0.4626 - val_acc: 0.8944\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1184 - acc: 0.9877 - val_loss: 0.4677 - val_acc: 0.8962\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1164 - acc: 0.9890 - val_loss: 0.4637 - val_acc: 0.8948\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1170 - acc: 0.9892 - val_loss: 0.4603 - val_acc: 0.8979\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 77s 8ms/step - loss: 0.1180 - acc: 0.9886 - val_loss: 0.4682 - val_acc: 0.8931\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1173 - acc: 0.9886 - val_loss: 0.4654 - val_acc: 0.8966\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1188 - acc: 0.9882 - val_loss: 0.4693 - val_acc: 0.8997\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1149 - acc: 0.9901 - val_loss: 0.4673 - val_acc: 0.8935\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1184 - acc: 0.9881 - val_loss: 0.4643 - val_acc: 0.8970\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1187 - acc: 0.9875 - val_loss: 0.4778 - val_acc: 0.8847\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1195 - acc: 0.9865 - val_loss: 0.4624 - val_acc: 0.8948\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1162 - acc: 0.9885 - val_loss: 0.4670 - val_acc: 0.8957\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1183 - acc: 0.9864 - val_loss: 0.4608 - val_acc: 0.8970\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1169 - acc: 0.9870 - val_loss: 0.4654 - val_acc: 0.8953\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1179 - acc: 0.9884 - val_loss: 0.4605 - val_acc: 0.8979\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1177 - acc: 0.9881 - val_loss: 0.4638 - val_acc: 0.8953\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1170 - acc: 0.9880 - val_loss: 0.4648 - val_acc: 0.8939\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1163 - acc: 0.9878 - val_loss: 0.4669 - val_acc: 0.8922\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1140 - acc: 0.9877 - val_loss: 0.4585 - val_acc: 0.8939\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1200 - acc: 0.9876 - val_loss: 0.4615 - val_acc: 0.8957\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1157 - acc: 0.9882 - val_loss: 0.4701 - val_acc: 0.8922\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 77s 9ms/step - loss: 0.1205 - acc: 0.9861 - val_loss: 0.4639 - val_acc: 0.8975\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1156 - acc: 0.9894 - val_loss: 0.4741 - val_acc: 0.8909\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.1161 - acc: 0.9874 - val_loss: 0.4596 - val_acc: 0.8939\n"
     ]
    }
   ],
   "source": [
    "#optimizer_8_7 = optimizers.RMSprop(lr = 0.0002)\n",
    "#model_8.compile(loss = 'categorical_crossentropy',optimizer = optimizer_8_7,metrics = ['acc'])\n",
    "hs_8_7 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_7th\")#899723rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1127 - acc: 0.9892 - val_loss: 0.4652 - val_acc: 0.8957\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1105 - acc: 0.9911 - val_loss: 0.4572 - val_acc: 0.8957\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.1077 - acc: 0.9913 - val_loss: 0.4656 - val_acc: 0.8957\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.1124 - acc: 0.9892 - val_loss: 0.4623 - val_acc: 0.8970\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1069 - acc: 0.9914 - val_loss: 0.4595 - val_acc: 0.8957\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1074 - acc: 0.9914 - val_loss: 0.4570 - val_acc: 0.8979\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1062 - acc: 0.9907 - val_loss: 0.4584 - val_acc: 0.8970\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1098 - acc: 0.9899 - val_loss: 0.4624 - val_acc: 0.8988\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1083 - acc: 0.9909 - val_loss: 0.4609 - val_acc: 0.8988\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1137 - acc: 0.9885 - val_loss: 0.4590 - val_acc: 0.8966\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1078 - acc: 0.9901 - val_loss: 0.4571 - val_acc: 0.8966\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1069 - acc: 0.9906 - val_loss: 0.4592 - val_acc: 0.8984\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1083 - acc: 0.9888 - val_loss: 0.4595 - val_acc: 0.8984\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1094 - acc: 0.9901 - val_loss: 0.4610 - val_acc: 0.8975\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.1050 - acc: 0.9913 - val_loss: 0.4585 - val_acc: 0.8975\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 97s 11ms/step - loss: 0.1090 - acc: 0.9894 - val_loss: 0.4574 - val_acc: 0.8953\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 95s 10ms/step - loss: 0.1057 - acc: 0.9907 - val_loss: 0.4571 - val_acc: 0.8984\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 95s 10ms/step - loss: 0.1053 - acc: 0.9923 - val_loss: 0.4619 - val_acc: 0.8957\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 93s 10ms/step - loss: 0.1074 - acc: 0.9909 - val_loss: 0.4622 - val_acc: 0.8970\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 95s 10ms/step - loss: 0.1063 - acc: 0.9905 - val_loss: 0.4621 - val_acc: 0.8970\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 97s 11ms/step - loss: 0.1070 - acc: 0.9904 - val_loss: 0.4599 - val_acc: 0.8962\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 93s 10ms/step - loss: 0.1090 - acc: 0.9898 - val_loss: 0.4583 - val_acc: 0.8966\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 100s 11ms/step - loss: 0.1066 - acc: 0.9916 - val_loss: 0.4565 - val_acc: 0.8966\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 105s 12ms/step - loss: 0.1051 - acc: 0.9917 - val_loss: 0.4646 - val_acc: 0.8931\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 103s 11ms/step - loss: 0.1068 - acc: 0.9908 - val_loss: 0.4553 - val_acc: 0.8992\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 100s 11ms/step - loss: 0.1065 - acc: 0.9907 - val_loss: 0.4558 - val_acc: 0.8984\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 102s 11ms/step - loss: 0.1068 - acc: 0.9917 - val_loss: 0.4637 - val_acc: 0.8975\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 99s 11ms/step - loss: 0.1064 - acc: 0.9881 - val_loss: 0.4592 - val_acc: 0.8970\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 103s 11ms/step - loss: 0.1060 - acc: 0.9915 - val_loss: 0.4605 - val_acc: 0.9015\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 99s 11ms/step - loss: 0.1039 - acc: 0.9916 - val_loss: 0.4647 - val_acc: 0.8962\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 102s 11ms/step - loss: 0.1098 - acc: 0.9897 - val_loss: 0.4582 - val_acc: 0.8957\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 96s 11ms/step - loss: 0.0987 - acc: 0.9931 - val_loss: 0.4554 - val_acc: 0.9006\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 100s 11ms/step - loss: 0.1040 - acc: 0.9919 - val_loss: 0.4561 - val_acc: 0.8939\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 103s 11ms/step - loss: 0.1048 - acc: 0.9909 - val_loss: 0.4579 - val_acc: 0.8962\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 99s 11ms/step - loss: 0.1051 - acc: 0.9909 - val_loss: 0.4554 - val_acc: 0.9010\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 94s 10ms/step - loss: 0.1036 - acc: 0.9908 - val_loss: 0.4566 - val_acc: 0.9001\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 99s 11ms/step - loss: 0.1054 - acc: 0.9915 - val_loss: 0.4568 - val_acc: 0.8975\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 97s 11ms/step - loss: 0.1097 - acc: 0.9893 - val_loss: 0.4545 - val_acc: 0.8988\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 97s 11ms/step - loss: 0.1060 - acc: 0.9904 - val_loss: 0.4557 - val_acc: 0.9006\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 95s 10ms/step - loss: 0.1059 - acc: 0.9903 - val_loss: 0.4595 - val_acc: 0.8975\n"
     ]
    }
   ],
   "source": [
    "optimizer_8_8 = optimizers.RMSprop(lr = 0.0002)\n",
    "model_8.compile(loss = 'categorical_crossentropy',optimizer = optimizer_8_8,metrics = ['acc'])\n",
    "hs_8_8 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_8th\")#901529th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = load_model(\"TextCNN_option_8_8th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 73s 8ms/step - loss: 0.1024 - acc: 0.9929 - val_loss: 0.4590 - val_acc: 0.8953\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1052 - acc: 0.9917 - val_loss: 0.4545 - val_acc: 0.8988\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 74s 8ms/step - loss: 0.1032 - acc: 0.9920 - val_loss: 0.4610 - val_acc: 0.8957\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1062 - acc: 0.9902 - val_loss: 0.4552 - val_acc: 0.8979\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1045 - acc: 0.9894 - val_loss: 0.4546 - val_acc: 0.8957\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1039 - acc: 0.9925 - val_loss: 0.4541 - val_acc: 0.8970\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1047 - acc: 0.9909 - val_loss: 0.4597 - val_acc: 0.8966\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1049 - acc: 0.9914 - val_loss: 0.4546 - val_acc: 0.8935\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1046 - acc: 0.9905 - val_loss: 0.4607 - val_acc: 0.8962\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.1059 - acc: 0.9888 - val_loss: 0.4595 - val_acc: 0.8962\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1028 - acc: 0.9911 - val_loss: 0.4577 - val_acc: 0.8953\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1093 - acc: 0.9897 - val_loss: 0.4635 - val_acc: 0.8962\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1057 - acc: 0.9898 - val_loss: 0.4604 - val_acc: 0.8962\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 75s 8ms/step - loss: 0.1034 - acc: 0.9916 - val_loss: 0.4566 - val_acc: 0.8948\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1062 - acc: 0.9907 - val_loss: 0.4593 - val_acc: 0.8966\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1030 - acc: 0.9908 - val_loss: 0.4619 - val_acc: 0.8939\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1061 - acc: 0.9896 - val_loss: 0.4605 - val_acc: 0.8984\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1015 - acc: 0.9927 - val_loss: 0.4569 - val_acc: 0.8957\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1025 - acc: 0.9915 - val_loss: 0.4554 - val_acc: 0.8975\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1036 - acc: 0.9906 - val_loss: 0.4566 - val_acc: 0.8966\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1017 - acc: 0.9923 - val_loss: 0.4535 - val_acc: 0.8962\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1020 - acc: 0.9909 - val_loss: 0.4570 - val_acc: 0.8948\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1035 - acc: 0.9898 - val_loss: 0.4611 - val_acc: 0.8939\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1009 - acc: 0.9913 - val_loss: 0.4602 - val_acc: 0.8944\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1004 - acc: 0.9923 - val_loss: 0.4513 - val_acc: 0.8970\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 77s 9ms/step - loss: 0.1011 - acc: 0.9914 - val_loss: 0.4574 - val_acc: 0.8975\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1035 - acc: 0.9916 - val_loss: 0.4526 - val_acc: 0.8970\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 77s 9ms/step - loss: 0.1025 - acc: 0.9915 - val_loss: 0.4538 - val_acc: 0.8988\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0998 - acc: 0.9925 - val_loss: 0.4519 - val_acc: 0.8970\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1021 - acc: 0.9918 - val_loss: 0.4519 - val_acc: 0.8997\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1030 - acc: 0.9913 - val_loss: 0.4568 - val_acc: 0.9001\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1007 - acc: 0.9924 - val_loss: 0.4568 - val_acc: 0.8975\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1032 - acc: 0.9916 - val_loss: 0.4546 - val_acc: 0.9001\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1036 - acc: 0.9916 - val_loss: 0.4525 - val_acc: 0.9006\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1015 - acc: 0.9916 - val_loss: 0.4615 - val_acc: 0.8962\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 77s 8ms/step - loss: 0.1070 - acc: 0.9892 - val_loss: 0.4575 - val_acc: 0.8997\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 77s 8ms/step - loss: 0.1017 - acc: 0.9919 - val_loss: 0.4572 - val_acc: 0.8966\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1012 - acc: 0.9917 - val_loss: 0.4583 - val_acc: 0.8966\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.1004 - acc: 0.9914 - val_loss: 0.4556 - val_acc: 0.8944\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 77s 8ms/step - loss: 0.1019 - acc: 0.9908 - val_loss: 0.4589 - val_acc: 0.8997\n"
     ]
    }
   ],
   "source": [
    "hs_8_9 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_9th\")#loss 4513(25th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0978 - acc: 0.9935 - val_loss: 0.4539 - val_acc: 0.8992\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1018 - acc: 0.9905 - val_loss: 0.4551 - val_acc: 0.8970\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.1003 - acc: 0.9925 - val_loss: 0.4539 - val_acc: 0.8948\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1003 - acc: 0.9917 - val_loss: 0.4554 - val_acc: 0.8970\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 77s 8ms/step - loss: 0.1061 - acc: 0.9898 - val_loss: 0.4584 - val_acc: 0.8962\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 76s 8ms/step - loss: 0.0983 - acc: 0.9933 - val_loss: 0.4594 - val_acc: 0.8948\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0994 - acc: 0.9928 - val_loss: 0.4551 - val_acc: 0.8970\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.1003 - acc: 0.9911 - val_loss: 0.4526 - val_acc: 0.8962\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1010 - acc: 0.9923 - val_loss: 0.4544 - val_acc: 0.8970\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0997 - acc: 0.9930 - val_loss: 0.4547 - val_acc: 0.8962\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1041 - acc: 0.9908 - val_loss: 0.4539 - val_acc: 0.8962\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0977 - acc: 0.9936 - val_loss: 0.4534 - val_acc: 0.9001\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0993 - acc: 0.9922 - val_loss: 0.4538 - val_acc: 0.8970\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1013 - acc: 0.9915 - val_loss: 0.4552 - val_acc: 0.8962\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1015 - acc: 0.9914 - val_loss: 0.4554 - val_acc: 0.8957\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.1036 - acc: 0.9906 - val_loss: 0.4551 - val_acc: 0.8970\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1038 - acc: 0.9894 - val_loss: 0.4572 - val_acc: 0.8948\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0995 - acc: 0.9923 - val_loss: 0.4559 - val_acc: 0.8970\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1007 - acc: 0.9911 - val_loss: 0.4554 - val_acc: 0.8984\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 72s 8ms/step - loss: 0.1017 - acc: 0.9913 - val_loss: 0.4555 - val_acc: 0.8953\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 71s 8ms/step - loss: 0.0980 - acc: 0.9924 - val_loss: 0.4582 - val_acc: 0.8962\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1006 - acc: 0.9918 - val_loss: 0.4525 - val_acc: 0.8966\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1005 - acc: 0.9916 - val_loss: 0.4553 - val_acc: 0.8962\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 68s 7ms/step - loss: 0.1013 - acc: 0.9915 - val_loss: 0.4557 - val_acc: 0.8975\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1020 - acc: 0.9915 - val_loss: 0.4568 - val_acc: 0.8962\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.0994 - acc: 0.9922 - val_loss: 0.4538 - val_acc: 0.8966\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.0994 - acc: 0.9915 - val_loss: 0.4554 - val_acc: 0.8992\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1012 - acc: 0.9918 - val_loss: 0.4538 - val_acc: 0.8970\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.1014 - acc: 0.9926 - val_loss: 0.4550 - val_acc: 0.8962\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 67s 7ms/step - loss: 0.0972 - acc: 0.9928 - val_loss: 0.4565 - val_acc: 0.8966\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.1028 - acc: 0.9915 - val_loss: 0.4563 - val_acc: 0.8966\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0984 - acc: 0.9923 - val_loss: 0.4542 - val_acc: 0.8948\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 68s 8ms/step - loss: 0.0964 - acc: 0.9940 - val_loss: 0.4546 - val_acc: 0.8962\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0982 - acc: 0.9929 - val_loss: 0.4541 - val_acc: 0.8944\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0990 - acc: 0.9924 - val_loss: 0.4554 - val_acc: 0.8992\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1026 - acc: 0.9907 - val_loss: 0.4577 - val_acc: 0.8975\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.1014 - acc: 0.9902 - val_loss: 0.4550 - val_acc: 0.8997\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0990 - acc: 0.9933 - val_loss: 0.4538 - val_acc: 0.8988\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 70s 8ms/step - loss: 0.0999 - acc: 0.9919 - val_loss: 0.4539 - val_acc: 0.8975\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 69s 8ms/step - loss: 0.0964 - acc: 0.9933 - val_loss: 0.4581 - val_acc: 0.8966\n"
     ]
    }
   ],
   "source": [
    "optimizer_8_10 = optimizers.RMSprop(lr = 0.0001)\n",
    "model_8.compile(loss = 'categorical_crossentropy',optimizer = optimizer_8_10,metrics = ['acc'])\n",
    "hs_8_10 = model_8.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.save(\"TextCNN_option_8_10th\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth update --- change filter_sizers to (2,3,4,5), decrease lambda of L2 regularizer to 1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 3.7011 - acc: 0.1893 - val_loss: 1.8646 - val_acc: 0.5842\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 1.7707 - acc: 0.4675 - val_loss: 1.3937 - val_acc: 0.7030\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 1.2681 - acc: 0.5993 - val_loss: 1.1627 - val_acc: 0.7596\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.9994 - acc: 0.6875 - val_loss: 1.0135 - val_acc: 0.7839\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.8547 - acc: 0.7262 - val_loss: 0.9193 - val_acc: 0.7994\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.7534 - acc: 0.7628 - val_loss: 0.8177 - val_acc: 0.8312\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.6688 - acc: 0.7935 - val_loss: 0.7784 - val_acc: 0.8272\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.6069 - acc: 0.8078 - val_loss: 0.7473 - val_acc: 0.8219\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.5645 - acc: 0.8258 - val_loss: 0.6924 - val_acc: 0.8361\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.5191 - acc: 0.8386 - val_loss: 0.6674 - val_acc: 0.8396\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.4841 - acc: 0.8508 - val_loss: 0.6278 - val_acc: 0.8586\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.4469 - acc: 0.8649 - val_loss: 0.6007 - val_acc: 0.8582\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.4107 - acc: 0.8725 - val_loss: 0.5687 - val_acc: 0.8772\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.3839 - acc: 0.8813 - val_loss: 0.5621 - val_acc: 0.8745\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.3730 - acc: 0.8866 - val_loss: 0.5399 - val_acc: 0.8648\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.3502 - acc: 0.8925 - val_loss: 0.5251 - val_acc: 0.8719\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.3239 - acc: 0.9020 - val_loss: 0.5100 - val_acc: 0.8785\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.3060 - acc: 0.9063 - val_loss: 0.5122 - val_acc: 0.8749\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.3052 - acc: 0.9105 - val_loss: 0.4972 - val_acc: 0.8701\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.2885 - acc: 0.9114 - val_loss: 0.4876 - val_acc: 0.8714\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.2756 - acc: 0.9206 - val_loss: 0.4725 - val_acc: 0.8847\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.2600 - acc: 0.9221 - val_loss: 0.4813 - val_acc: 0.8723\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.2468 - acc: 0.9297 - val_loss: 0.4673 - val_acc: 0.8794\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.2506 - acc: 0.9275 - val_loss: 0.4588 - val_acc: 0.8758\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.2328 - acc: 0.9330 - val_loss: 0.4572 - val_acc: 0.8732\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.2321 - acc: 0.9270 - val_loss: 0.4458 - val_acc: 0.8776\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.2184 - acc: 0.9361 - val_loss: 0.4429 - val_acc: 0.8798\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.2080 - acc: 0.9395 - val_loss: 0.4539 - val_acc: 0.8719\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.2073 - acc: 0.9427 - val_loss: 0.4268 - val_acc: 0.8838\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1986 - acc: 0.9431 - val_loss: 0.4298 - val_acc: 0.8798\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1933 - acc: 0.9453 - val_loss: 0.4232 - val_acc: 0.8776\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1891 - acc: 0.9464 - val_loss: 0.4222 - val_acc: 0.8816\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1851 - acc: 0.9473 - val_loss: 0.4292 - val_acc: 0.8780\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1752 - acc: 0.9512 - val_loss: 0.4358 - val_acc: 0.8820\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1776 - acc: 0.9494 - val_loss: 0.4231 - val_acc: 0.8820\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1794 - acc: 0.9508 - val_loss: 0.4126 - val_acc: 0.8864\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.1698 - acc: 0.9549 - val_loss: 0.4113 - val_acc: 0.8856\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.1643 - acc: 0.9551 - val_loss: 0.4118 - val_acc: 0.8833\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.1656 - acc: 0.9534 - val_loss: 0.4068 - val_acc: 0.8860\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1631 - acc: 0.9558 - val_loss: 0.4198 - val_acc: 0.8811\n"
     ]
    }
   ],
   "source": [
    "in_x = Input(shape = (X_train.shape[1],X_train.shape[2]),dtype = 'float32')\n",
    "\n",
    "input_x = GaussianNoise(stddev = 0.01)(in_x)\n",
    "\n",
    "x1 = Conv1D(128,2,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "x2 = Conv1D(128,3,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "x3 = Conv1D(128,4,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "\n",
    "x4 = Conv1D(128,5,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x4 = Dropout(0.5)(x4)\n",
    "\n",
    "x1 = GlobalMaxPooling1D()(x1)# 128 dim vector\n",
    "x2 = GlobalMaxPooling1D()(x2) # 128 dim vector\n",
    "x3 = GlobalMaxPooling1D()(x3) # 128 dim vector\n",
    "x4 = GlobalMaxPooling1D()(x4) # 128 dim vector\n",
    "\n",
    "\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2,x3,x4])\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='softmax',kernel_regularizer = regularizers.l2(1e-4))(x)\n",
    "\n",
    "model_10 = Model(in_x,preds)\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_10.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "hs_10_1 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_1st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1554 - acc: 0.9574 - val_loss: 0.4205 - val_acc: 0.8767\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1543 - acc: 0.9571 - val_loss: 0.4091 - val_acc: 0.8816\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1538 - acc: 0.9589 - val_loss: 0.4193 - val_acc: 0.8798\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1430 - acc: 0.9629 - val_loss: 0.4147 - val_acc: 0.8851\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1528 - acc: 0.9586 - val_loss: 0.4129 - val_acc: 0.8847\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1488 - acc: 0.9595 - val_loss: 0.4052 - val_acc: 0.8816\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1481 - acc: 0.9578 - val_loss: 0.4081 - val_acc: 0.8829\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1397 - acc: 0.9622 - val_loss: 0.4104 - val_acc: 0.8851\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1430 - acc: 0.9622 - val_loss: 0.4116 - val_acc: 0.8856\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1294 - acc: 0.9651 - val_loss: 0.4052 - val_acc: 0.8909\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1330 - acc: 0.9627 - val_loss: 0.4063 - val_acc: 0.8878\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1328 - acc: 0.9663 - val_loss: 0.4017 - val_acc: 0.8904\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1328 - acc: 0.9661 - val_loss: 0.4141 - val_acc: 0.8873\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1315 - acc: 0.9655 - val_loss: 0.4259 - val_acc: 0.8851\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 103s 11ms/step - loss: 0.1241 - acc: 0.9688 - val_loss: 0.4016 - val_acc: 0.8957\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 97s 11ms/step - loss: 0.1284 - acc: 0.9666 - val_loss: 0.4067 - val_acc: 0.8904\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1288 - acc: 0.9642 - val_loss: 0.4036 - val_acc: 0.8904\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1187 - acc: 0.9681 - val_loss: 0.4017 - val_acc: 0.8882\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1137 - acc: 0.9707 - val_loss: 0.4076 - val_acc: 0.8864\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1274 - acc: 0.9665 - val_loss: 0.3988 - val_acc: 0.8922\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1196 - acc: 0.9698 - val_loss: 0.4017 - val_acc: 0.8873\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1217 - acc: 0.9705 - val_loss: 0.4085 - val_acc: 0.8842\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.1191 - acc: 0.9726 - val_loss: 0.4055 - val_acc: 0.8864\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.1206 - acc: 0.9703 - val_loss: 0.3968 - val_acc: 0.8886\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1156 - acc: 0.9709 - val_loss: 0.4079 - val_acc: 0.8860\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1164 - acc: 0.9732 - val_loss: 0.3994 - val_acc: 0.8886\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1088 - acc: 0.9734 - val_loss: 0.4129 - val_acc: 0.8811\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1103 - acc: 0.9739 - val_loss: 0.4055 - val_acc: 0.8939\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1136 - acc: 0.9713 - val_loss: 0.4122 - val_acc: 0.8895\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1142 - acc: 0.9712 - val_loss: 0.4101 - val_acc: 0.8935\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1039 - acc: 0.9751 - val_loss: 0.3997 - val_acc: 0.8891\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1110 - acc: 0.9717 - val_loss: 0.4105 - val_acc: 0.8816\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1095 - acc: 0.9712 - val_loss: 0.4170 - val_acc: 0.8833\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.1058 - acc: 0.9738 - val_loss: 0.4038 - val_acc: 0.8856\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1077 - acc: 0.9724 - val_loss: 0.4188 - val_acc: 0.8851\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.1032 - acc: 0.9740 - val_loss: 0.4081 - val_acc: 0.8886\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1008 - acc: 0.9757 - val_loss: 0.4013 - val_acc: 0.8895\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.1022 - acc: 0.9758 - val_loss: 0.4124 - val_acc: 0.8856\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1064 - acc: 0.9759 - val_loss: 0.4172 - val_acc: 0.8856\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.1043 - acc: 0.9771 - val_loss: 0.3966 - val_acc: 0.8886\n"
     ]
    }
   ],
   "source": [
    "hs_10_2 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_2nd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.1014 - acc: 0.9776 - val_loss: 0.3981 - val_acc: 0.8904\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0935 - acc: 0.9789 - val_loss: 0.3929 - val_acc: 0.8891\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0881 - acc: 0.9808 - val_loss: 0.4033 - val_acc: 0.8935\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0839 - acc: 0.9811 - val_loss: 0.4008 - val_acc: 0.8913\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0870 - acc: 0.9817 - val_loss: 0.3938 - val_acc: 0.8935\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0883 - acc: 0.9804 - val_loss: 0.4055 - val_acc: 0.8922\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0814 - acc: 0.9819 - val_loss: 0.3980 - val_acc: 0.8904\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0828 - acc: 0.9828 - val_loss: 0.4006 - val_acc: 0.8891\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0830 - acc: 0.9817 - val_loss: 0.3994 - val_acc: 0.8948\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0792 - acc: 0.9844 - val_loss: 0.3997 - val_acc: 0.8922\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0777 - acc: 0.9841 - val_loss: 0.3875 - val_acc: 0.8931\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0847 - acc: 0.9810 - val_loss: 0.4020 - val_acc: 0.8891\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0775 - acc: 0.9854 - val_loss: 0.4075 - val_acc: 0.8891\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0715 - acc: 0.9860 - val_loss: 0.3959 - val_acc: 0.8939\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0742 - acc: 0.9854 - val_loss: 0.3942 - val_acc: 0.8931\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0750 - acc: 0.9842 - val_loss: 0.4043 - val_acc: 0.8886\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0765 - acc: 0.9856 - val_loss: 0.4048 - val_acc: 0.8917\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0737 - acc: 0.9859 - val_loss: 0.3918 - val_acc: 0.8935\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0745 - acc: 0.9854 - val_loss: 0.4056 - val_acc: 0.8900\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0737 - acc: 0.9862 - val_loss: 0.4022 - val_acc: 0.8922\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0726 - acc: 0.9860 - val_loss: 0.3896 - val_acc: 0.8970\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0701 - acc: 0.9855 - val_loss: 0.3987 - val_acc: 0.8957\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0725 - acc: 0.9853 - val_loss: 0.3882 - val_acc: 0.8922\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0757 - acc: 0.9848 - val_loss: 0.4040 - val_acc: 0.8926\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0793 - acc: 0.9845 - val_loss: 0.4012 - val_acc: 0.8922\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0721 - acc: 0.9865 - val_loss: 0.3944 - val_acc: 0.8962\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0677 - acc: 0.9883 - val_loss: 0.3973 - val_acc: 0.8935\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0709 - acc: 0.9872 - val_loss: 0.3998 - val_acc: 0.8957\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0697 - acc: 0.9867 - val_loss: 0.4084 - val_acc: 0.8957\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0704 - acc: 0.9860 - val_loss: 0.4098 - val_acc: 0.8904\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0670 - acc: 0.9877 - val_loss: 0.4027 - val_acc: 0.8917\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0640 - acc: 0.9890 - val_loss: 0.3974 - val_acc: 0.8966\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0671 - acc: 0.9872 - val_loss: 0.4002 - val_acc: 0.8931\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0658 - acc: 0.9881 - val_loss: 0.4097 - val_acc: 0.8935\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0654 - acc: 0.9877 - val_loss: 0.4044 - val_acc: 0.8966\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0668 - acc: 0.9881 - val_loss: 0.4028 - val_acc: 0.8931\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0695 - acc: 0.9869 - val_loss: 0.4020 - val_acc: 0.8900\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0674 - acc: 0.9890 - val_loss: 0.4010 - val_acc: 0.8948\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0704 - acc: 0.9881 - val_loss: 0.4115 - val_acc: 0.8935\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0674 - acc: 0.9875 - val_loss: 0.4084 - val_acc: 0.8904\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_10.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "hs_10_3 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_3rd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0680 - acc: 0.9878 - val_loss: 0.4010 - val_acc: 0.8931\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0664 - acc: 0.9876 - val_loss: 0.3979 - val_acc: 0.8909\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0639 - acc: 0.9888 - val_loss: 0.3998 - val_acc: 0.8948\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0680 - acc: 0.9882 - val_loss: 0.4053 - val_acc: 0.8922\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0670 - acc: 0.9871 - val_loss: 0.4026 - val_acc: 0.8953\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0646 - acc: 0.9890 - val_loss: 0.3990 - val_acc: 0.8984\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0595 - acc: 0.9901 - val_loss: 0.4035 - val_acc: 0.8953\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0691 - acc: 0.9860 - val_loss: 0.3983 - val_acc: 0.8909\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0680 - acc: 0.9883 - val_loss: 0.4049 - val_acc: 0.8935\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0659 - acc: 0.9884 - val_loss: 0.4065 - val_acc: 0.8913\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0665 - acc: 0.9893 - val_loss: 0.4069 - val_acc: 0.8922\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0643 - acc: 0.9891 - val_loss: 0.4049 - val_acc: 0.8944\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0602 - acc: 0.9904 - val_loss: 0.4017 - val_acc: 0.8948\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0679 - acc: 0.9873 - val_loss: 0.3966 - val_acc: 0.8966\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0626 - acc: 0.9890 - val_loss: 0.3910 - val_acc: 0.8975\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0673 - acc: 0.9872 - val_loss: 0.3948 - val_acc: 0.8988\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0633 - acc: 0.9885 - val_loss: 0.3949 - val_acc: 0.8953\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0612 - acc: 0.9894 - val_loss: 0.3896 - val_acc: 0.8975\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0678 - acc: 0.9867 - val_loss: 0.3963 - val_acc: 0.8962\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0648 - acc: 0.9887 - val_loss: 0.3941 - val_acc: 0.8957\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0664 - acc: 0.9869 - val_loss: 0.4091 - val_acc: 0.8962\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0652 - acc: 0.9890 - val_loss: 0.4018 - val_acc: 0.8935\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0649 - acc: 0.9883 - val_loss: 0.3915 - val_acc: 0.8957\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0650 - acc: 0.9882 - val_loss: 0.3908 - val_acc: 0.9001\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0607 - acc: 0.9896 - val_loss: 0.3916 - val_acc: 0.8975\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0596 - acc: 0.9898 - val_loss: 0.3927 - val_acc: 0.8992\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0618 - acc: 0.9891 - val_loss: 0.3947 - val_acc: 0.8975\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0635 - acc: 0.9882 - val_loss: 0.4042 - val_acc: 0.8953\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0619 - acc: 0.9887 - val_loss: 0.4002 - val_acc: 0.8966\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0621 - acc: 0.9903 - val_loss: 0.3987 - val_acc: 0.8948\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0594 - acc: 0.9901 - val_loss: 0.3991 - val_acc: 0.8975\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0613 - acc: 0.9899 - val_loss: 0.3959 - val_acc: 0.8962\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0606 - acc: 0.9895 - val_loss: 0.3963 - val_acc: 0.8984\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0636 - acc: 0.9897 - val_loss: 0.3962 - val_acc: 0.8909\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0589 - acc: 0.9893 - val_loss: 0.3989 - val_acc: 0.8953\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0598 - acc: 0.9915 - val_loss: 0.4003 - val_acc: 0.8935\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0613 - acc: 0.9877 - val_loss: 0.3936 - val_acc: 0.9001\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0625 - acc: 0.9890 - val_loss: 0.4004 - val_acc: 0.8970\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0617 - acc: 0.9884 - val_loss: 0.3892 - val_acc: 0.8992\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0585 - acc: 0.9906 - val_loss: 0.3982 - val_acc: 0.8957\n"
     ]
    }
   ],
   "source": [
    "hs_10_4 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_4th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0572 - acc: 0.9911 - val_loss: 0.3927 - val_acc: 0.8997\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0578 - acc: 0.9904 - val_loss: 0.3936 - val_acc: 0.8992\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0610 - acc: 0.9892 - val_loss: 0.3942 - val_acc: 0.8984\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0567 - acc: 0.9912 - val_loss: 0.4001 - val_acc: 0.8984\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0571 - acc: 0.9911 - val_loss: 0.3965 - val_acc: 0.8926\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0568 - acc: 0.9902 - val_loss: 0.3960 - val_acc: 0.8953\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0531 - acc: 0.9919 - val_loss: 0.3968 - val_acc: 0.8962\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0550 - acc: 0.9916 - val_loss: 0.3969 - val_acc: 0.8939\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0534 - acc: 0.9923 - val_loss: 0.3945 - val_acc: 0.8953\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0539 - acc: 0.9922 - val_loss: 0.3955 - val_acc: 0.8979\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0562 - acc: 0.9917 - val_loss: 0.3944 - val_acc: 0.9006\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0559 - acc: 0.9905 - val_loss: 0.3874 - val_acc: 0.8975\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0529 - acc: 0.9922 - val_loss: 0.3948 - val_acc: 0.8984\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0547 - acc: 0.9912 - val_loss: 0.3921 - val_acc: 0.8966\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0512 - acc: 0.9924 - val_loss: 0.3958 - val_acc: 0.9006\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0540 - acc: 0.9924 - val_loss: 0.3955 - val_acc: 0.8997\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0532 - acc: 0.9911 - val_loss: 0.3953 - val_acc: 0.8997\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0558 - acc: 0.9928 - val_loss: 0.3912 - val_acc: 0.8979\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0510 - acc: 0.9935 - val_loss: 0.3937 - val_acc: 0.8984\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0511 - acc: 0.9917 - val_loss: 0.3933 - val_acc: 0.9001\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0546 - acc: 0.9911 - val_loss: 0.3974 - val_acc: 0.8979\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0537 - acc: 0.9922 - val_loss: 0.3954 - val_acc: 0.8975\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0540 - acc: 0.9922 - val_loss: 0.3866 - val_acc: 0.8984\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0537 - acc: 0.9917 - val_loss: 0.3925 - val_acc: 0.9001\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0501 - acc: 0.9923 - val_loss: 0.3910 - val_acc: 0.8992\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0535 - acc: 0.9913 - val_loss: 0.3930 - val_acc: 0.8988\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0537 - acc: 0.9920 - val_loss: 0.3908 - val_acc: 0.8988\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0545 - acc: 0.9909 - val_loss: 0.3860 - val_acc: 0.9006\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0525 - acc: 0.9924 - val_loss: 0.3899 - val_acc: 0.8992\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0513 - acc: 0.9925 - val_loss: 0.3856 - val_acc: 0.9041\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0506 - acc: 0.9927 - val_loss: 0.3914 - val_acc: 0.9015\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0549 - acc: 0.9912 - val_loss: 0.3899 - val_acc: 0.9001\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0480 - acc: 0.9934 - val_loss: 0.3891 - val_acc: 0.9015\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0493 - acc: 0.9937 - val_loss: 0.3935 - val_acc: 0.8984\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0502 - acc: 0.9930 - val_loss: 0.3921 - val_acc: 0.9010\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 99s 11ms/step - loss: 0.0487 - acc: 0.9938 - val_loss: 0.3891 - val_acc: 0.9001\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 100s 11ms/step - loss: 0.0483 - acc: 0.9945 - val_loss: 0.3939 - val_acc: 0.8979\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 102s 11ms/step - loss: 0.0533 - acc: 0.9916 - val_loss: 0.3881 - val_acc: 0.9023\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 101s 11ms/step - loss: 0.0487 - acc: 0.9943 - val_loss: 0.3912 - val_acc: 0.9023\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 94s 10ms/step - loss: 0.0492 - acc: 0.9934 - val_loss: 0.3936 - val_acc: 0.9010\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.0002, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_10.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "hs_10_5 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_5th\")#9041(30th) loss= 3856(30th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0478 - acc: 0.9931 - val_loss: 0.3947 - val_acc: 0.8970\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0506 - acc: 0.9933 - val_loss: 0.3918 - val_acc: 0.9010\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0517 - acc: 0.9931 - val_loss: 0.3918 - val_acc: 0.8975\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0488 - acc: 0.9928 - val_loss: 0.3955 - val_acc: 0.9001\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0497 - acc: 0.9930 - val_loss: 0.3952 - val_acc: 0.9006\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0483 - acc: 0.9933 - val_loss: 0.3954 - val_acc: 0.8988\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0515 - acc: 0.9920 - val_loss: 0.3925 - val_acc: 0.8975\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0520 - acc: 0.9924 - val_loss: 0.3921 - val_acc: 0.9010\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0487 - acc: 0.9940 - val_loss: 0.3911 - val_acc: 0.9015\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0448 - acc: 0.9945 - val_loss: 0.3941 - val_acc: 0.9010\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0504 - acc: 0.9937 - val_loss: 0.3944 - val_acc: 0.9028\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0507 - acc: 0.9930 - val_loss: 0.3887 - val_acc: 0.9019\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0488 - acc: 0.9926 - val_loss: 0.3891 - val_acc: 0.9028\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0557 - acc: 0.9920 - val_loss: 0.3879 - val_acc: 0.9019\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0494 - acc: 0.9941 - val_loss: 0.4003 - val_acc: 0.8975\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0467 - acc: 0.9941 - val_loss: 0.3961 - val_acc: 0.9010\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0480 - acc: 0.9944 - val_loss: 0.3956 - val_acc: 0.9006\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0441 - acc: 0.9952 - val_loss: 0.3963 - val_acc: 0.9006\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0472 - acc: 0.9936 - val_loss: 0.3929 - val_acc: 0.8984\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0439 - acc: 0.9960 - val_loss: 0.3915 - val_acc: 0.9015\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0527 - acc: 0.9922 - val_loss: 0.3922 - val_acc: 0.8975\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0474 - acc: 0.9944 - val_loss: 0.3932 - val_acc: 0.9001\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0482 - acc: 0.9941 - val_loss: 0.3904 - val_acc: 0.8997\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0485 - acc: 0.9931 - val_loss: 0.3872 - val_acc: 0.9001\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0481 - acc: 0.9934 - val_loss: 0.3940 - val_acc: 0.8992\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0485 - acc: 0.9938 - val_loss: 0.3903 - val_acc: 0.9001\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0482 - acc: 0.9938 - val_loss: 0.3928 - val_acc: 0.8979\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0473 - acc: 0.9929 - val_loss: 0.3883 - val_acc: 0.9032\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0461 - acc: 0.9941 - val_loss: 0.3909 - val_acc: 0.9006\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0479 - acc: 0.9936 - val_loss: 0.3949 - val_acc: 0.8997\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0494 - acc: 0.9931 - val_loss: 0.3928 - val_acc: 0.8997\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0458 - acc: 0.9946 - val_loss: 0.3959 - val_acc: 0.8984\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0470 - acc: 0.9934 - val_loss: 0.3951 - val_acc: 0.8979\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0467 - acc: 0.9938 - val_loss: 0.3924 - val_acc: 0.9010\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0465 - acc: 0.9944 - val_loss: 0.3877 - val_acc: 0.9019\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0513 - acc: 0.9933 - val_loss: 0.3933 - val_acc: 0.9015\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0480 - acc: 0.9930 - val_loss: 0.3919 - val_acc: 0.9006\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 92s 10ms/step - loss: 0.0466 - acc: 0.9947 - val_loss: 0.3900 - val_acc: 0.8997\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0460 - acc: 0.9943 - val_loss: 0.3941 - val_acc: 0.8979\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0494 - acc: 0.9929 - val_loss: 0.3908 - val_acc: 0.9010\n"
     ]
    }
   ],
   "source": [
    "hs_10_6 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_6th\") #9032(28th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0475 - acc: 0.9936 - val_loss: 0.3914 - val_acc: 0.8992\n",
      "Epoch 2/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0439 - acc: 0.9936 - val_loss: 0.3879 - val_acc: 0.9010\n",
      "Epoch 3/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0459 - acc: 0.9936 - val_loss: 0.3897 - val_acc: 0.9006\n",
      "Epoch 4/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0472 - acc: 0.9934 - val_loss: 0.3897 - val_acc: 0.9001\n",
      "Epoch 5/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0455 - acc: 0.9938 - val_loss: 0.3893 - val_acc: 0.8970\n",
      "Epoch 6/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0459 - acc: 0.9936 - val_loss: 0.3902 - val_acc: 0.8975\n",
      "Epoch 7/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0451 - acc: 0.9945 - val_loss: 0.3878 - val_acc: 0.8988\n",
      "Epoch 8/20\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0464 - acc: 0.9945 - val_loss: 0.3910 - val_acc: 0.8975\n",
      "Epoch 9/20\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0503 - acc: 0.9926 - val_loss: 0.3926 - val_acc: 0.8962\n",
      "Epoch 10/20\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0423 - acc: 0.9955 - val_loss: 0.3901 - val_acc: 0.8979\n",
      "Epoch 11/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0456 - acc: 0.9952 - val_loss: 0.3933 - val_acc: 0.8975\n",
      "Epoch 12/20\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0493 - acc: 0.9928 - val_loss: 0.3887 - val_acc: 0.9001\n",
      "Epoch 13/20\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0441 - acc: 0.9945 - val_loss: 0.3910 - val_acc: 0.8988\n",
      "Epoch 14/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0462 - acc: 0.9941 - val_loss: 0.3932 - val_acc: 0.8979\n",
      "Epoch 15/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0463 - acc: 0.9935 - val_loss: 0.3894 - val_acc: 0.8975\n",
      "Epoch 16/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0462 - acc: 0.9945 - val_loss: 0.3921 - val_acc: 0.8957\n",
      "Epoch 17/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0439 - acc: 0.9944 - val_loss: 0.3915 - val_acc: 0.8984\n",
      "Epoch 18/20\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0450 - acc: 0.9948 - val_loss: 0.3877 - val_acc: 0.9001\n",
      "Epoch 19/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0462 - acc: 0.9941 - val_loss: 0.3877 - val_acc: 0.9010\n",
      "Epoch 20/20\n",
      "9051/9051 [==============================] - 81s 9ms/step - loss: 0.0423 - acc: 0.9957 - val_loss: 0.3891 - val_acc: 0.8988\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_10.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "hs_10_7 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 20,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_7th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10 = load_model(\"TextCNN_option_10_7th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/20\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0464 - acc: 0.9933 - val_loss: 0.3874 - val_acc: 0.9006\n",
      "Epoch 2/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0455 - acc: 0.9941 - val_loss: 0.3894 - val_acc: 0.9023\n",
      "Epoch 3/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0443 - acc: 0.9944 - val_loss: 0.3877 - val_acc: 0.8997\n",
      "Epoch 4/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0470 - acc: 0.9936 - val_loss: 0.3880 - val_acc: 0.9006\n",
      "Epoch 5/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0431 - acc: 0.9945 - val_loss: 0.3890 - val_acc: 0.9010\n",
      "Epoch 6/20\n",
      "9051/9051 [==============================] - 80s 9ms/step - loss: 0.0432 - acc: 0.9954 - val_loss: 0.3871 - val_acc: 0.8988\n",
      "Epoch 7/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0468 - acc: 0.9934 - val_loss: 0.3868 - val_acc: 0.9010\n",
      "Epoch 8/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0442 - acc: 0.9946 - val_loss: 0.3891 - val_acc: 0.9001\n",
      "Epoch 9/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0465 - acc: 0.9939 - val_loss: 0.3897 - val_acc: 0.9010\n",
      "Epoch 10/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0451 - acc: 0.9944 - val_loss: 0.3891 - val_acc: 0.9010\n",
      "Epoch 11/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0428 - acc: 0.9947 - val_loss: 0.3882 - val_acc: 0.9006\n",
      "Epoch 12/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0478 - acc: 0.9931 - val_loss: 0.3871 - val_acc: 0.9001\n",
      "Epoch 13/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0449 - acc: 0.9935 - val_loss: 0.3884 - val_acc: 0.9015\n",
      "Epoch 14/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0437 - acc: 0.9936 - val_loss: 0.3911 - val_acc: 0.8992\n",
      "Epoch 15/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0478 - acc: 0.9940 - val_loss: 0.3898 - val_acc: 0.9006\n",
      "Epoch 16/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0446 - acc: 0.9943 - val_loss: 0.3916 - val_acc: 0.9015\n",
      "Epoch 17/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0428 - acc: 0.9951 - val_loss: 0.3905 - val_acc: 0.9023\n",
      "Epoch 18/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0455 - acc: 0.9938 - val_loss: 0.3897 - val_acc: 0.9010\n",
      "Epoch 19/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0437 - acc: 0.9947 - val_loss: 0.3895 - val_acc: 0.9001\n",
      "Epoch 20/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0455 - acc: 0.9931 - val_loss: 0.3901 - val_acc: 0.9019\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_10.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "hs_10_8 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 20,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_8th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0461 - acc: 0.9941 - val_loss: 0.3903 - val_acc: 0.9015\n",
      "Epoch 2/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0423 - acc: 0.9952 - val_loss: 0.3901 - val_acc: 0.9010\n",
      "Epoch 3/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0456 - acc: 0.9940 - val_loss: 0.3903 - val_acc: 0.9010\n",
      "Epoch 4/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0446 - acc: 0.9944 - val_loss: 0.3910 - val_acc: 0.8984\n",
      "Epoch 5/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0437 - acc: 0.9944 - val_loss: 0.3904 - val_acc: 0.8988\n",
      "Epoch 6/20\n",
      "9051/9051 [==============================] - 77s 9ms/step - loss: 0.0423 - acc: 0.9958 - val_loss: 0.3892 - val_acc: 0.8988\n",
      "Epoch 7/20\n",
      "9051/9051 [==============================] - 82s 9ms/step - loss: 0.0424 - acc: 0.9958 - val_loss: 0.3879 - val_acc: 0.9006\n",
      "Epoch 8/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0435 - acc: 0.9948 - val_loss: 0.3877 - val_acc: 0.8997\n",
      "Epoch 9/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0454 - acc: 0.9929 - val_loss: 0.3873 - val_acc: 0.9001\n",
      "Epoch 10/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0432 - acc: 0.9944 - val_loss: 0.3888 - val_acc: 0.9006\n",
      "Epoch 11/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0428 - acc: 0.9959 - val_loss: 0.3896 - val_acc: 0.9019\n",
      "Epoch 12/20\n",
      "9051/9051 [==============================] - 78s 9ms/step - loss: 0.0460 - acc: 0.9945 - val_loss: 0.3874 - val_acc: 0.9010\n",
      "Epoch 13/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0428 - acc: 0.9945 - val_loss: 0.3892 - val_acc: 0.9028\n",
      "Epoch 14/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0439 - acc: 0.9941 - val_loss: 0.3892 - val_acc: 0.8992\n",
      "Epoch 15/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0412 - acc: 0.9961 - val_loss: 0.3893 - val_acc: 0.9006\n",
      "Epoch 16/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0454 - acc: 0.9938 - val_loss: 0.3896 - val_acc: 0.8992\n",
      "Epoch 17/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0425 - acc: 0.9962 - val_loss: 0.3886 - val_acc: 0.9015\n",
      "Epoch 18/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0472 - acc: 0.9936 - val_loss: 0.3889 - val_acc: 0.9019\n",
      "Epoch 19/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0435 - acc: 0.9948 - val_loss: 0.3881 - val_acc: 0.9019\n",
      "Epoch 20/20\n",
      "9051/9051 [==============================] - 79s 9ms/step - loss: 0.0434 - acc: 0.9945 - val_loss: 0.3872 - val_acc: 0.9006\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_10.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "hs_10_9 = model_10.fit(X_train,y_train,batch_size = 128,epochs = 20,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save(\"TextCNN_option_10_9th\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth update --- introduce GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 3.1009 - acc: 0.1979 - val_loss: 1.9432 - val_acc: 0.6138\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 1.6543 - acc: 0.4769 - val_loss: 1.4547 - val_acc: 0.7393\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 1.1926 - acc: 0.6211 - val_loss: 1.1911 - val_acc: 0.7508\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.9891 - acc: 0.6870 - val_loss: 1.0321 - val_acc: 0.7972\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.8366 - acc: 0.7379 - val_loss: 0.9463 - val_acc: 0.7799\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.7401 - acc: 0.7722 - val_loss: 0.8512 - val_acc: 0.8184\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.6737 - acc: 0.7932 - val_loss: 0.7849 - val_acc: 0.8365\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.6205 - acc: 0.8096 - val_loss: 0.7315 - val_acc: 0.8502\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.5552 - acc: 0.8325 - val_loss: 0.7147 - val_acc: 0.8414\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.5155 - acc: 0.8428 - val_loss: 0.6517 - val_acc: 0.8511\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.4955 - acc: 0.8503 - val_loss: 0.6342 - val_acc: 0.8489\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.4350 - acc: 0.8711 - val_loss: 0.5981 - val_acc: 0.8568\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.4044 - acc: 0.8805 - val_loss: 0.5904 - val_acc: 0.8529\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.3976 - acc: 0.8809 - val_loss: 0.5578 - val_acc: 0.8652\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.3638 - acc: 0.8901 - val_loss: 0.5414 - val_acc: 0.8701\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.3235 - acc: 0.9040 - val_loss: 0.5362 - val_acc: 0.8612\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.3181 - acc: 0.9023 - val_loss: 0.5084 - val_acc: 0.8674\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.3101 - acc: 0.9026 - val_loss: 0.4937 - val_acc: 0.8710\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.2960 - acc: 0.9101 - val_loss: 0.4929 - val_acc: 0.8714\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.2770 - acc: 0.9207 - val_loss: 0.4714 - val_acc: 0.8789\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.2685 - acc: 0.9219 - val_loss: 0.4793 - val_acc: 0.8723\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.2538 - acc: 0.9251 - val_loss: 0.4598 - val_acc: 0.8789\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.2371 - acc: 0.9306 - val_loss: 0.4500 - val_acc: 0.8825\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.2422 - acc: 0.9265 - val_loss: 0.4489 - val_acc: 0.8776\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.2200 - acc: 0.9361 - val_loss: 0.4423 - val_acc: 0.8798\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.2224 - acc: 0.9367 - val_loss: 0.4466 - val_acc: 0.8811\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.2148 - acc: 0.9353 - val_loss: 0.4333 - val_acc: 0.8820\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.2138 - acc: 0.9370 - val_loss: 0.4335 - val_acc: 0.8780\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1988 - acc: 0.9418 - val_loss: 0.4314 - val_acc: 0.8754\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1943 - acc: 0.9444 - val_loss: 0.4258 - val_acc: 0.8820\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.1934 - acc: 0.9437 - val_loss: 0.4183 - val_acc: 0.8825\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.1836 - acc: 0.9481 - val_loss: 0.4189 - val_acc: 0.8758\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.1791 - acc: 0.9509 - val_loss: 0.4179 - val_acc: 0.8749\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1733 - acc: 0.9517 - val_loss: 0.4136 - val_acc: 0.8794\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.1687 - acc: 0.9529 - val_loss: 0.4071 - val_acc: 0.8820\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 93s 10ms/step - loss: 0.1640 - acc: 0.9532 - val_loss: 0.4328 - val_acc: 0.8772\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1554 - acc: 0.9553 - val_loss: 0.4006 - val_acc: 0.8864\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1583 - acc: 0.9559 - val_loss: 0.4008 - val_acc: 0.8851\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1582 - acc: 0.9558 - val_loss: 0.4041 - val_acc: 0.8829\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.1571 - acc: 0.9540 - val_loss: 0.4132 - val_acc: 0.8816\n"
     ]
    }
   ],
   "source": [
    "in_x = Input(shape = (X_train.shape[1],X_train.shape[2]),dtype = 'float32')\n",
    "\n",
    "input_x = GaussianNoise(stddev = 0.01)(in_x)\n",
    "\n",
    "x1 = Conv1D(128,2,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "x2 = Conv1D(128,3,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "x3 = Conv1D(128,4,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "\n",
    "x4 = Conv1D(128,5,activation = 'relu')(input_x)#,kernel_regularizer = regularizers.l2(0.01)\n",
    "x4 = Dropout(0.5)(x4)\n",
    "\n",
    "x11 = GlobalMaxPooling1D()(x1)# 128 dim vector\n",
    "x12 = GlobalAveragePooling1D()(x1)# 128 dim vector\n",
    "\n",
    "x21 = GlobalMaxPooling1D()(x2) # 128 dim vector\n",
    "x22 = GlobalAveragePooling1D()(x2) # 128 dim vector\n",
    "\n",
    "x31 = GlobalMaxPooling1D()(x3) # 128 dim vector\n",
    "x32 = GlobalAveragePooling1D()(x3) # 128 dim vector\n",
    "\n",
    "x41 = GlobalMaxPooling1D()(x4) # 128 dim vector\n",
    "x42 = GlobalAveragePooling1D()(x4) # 128 dim vector\n",
    "\n",
    "\n",
    "\n",
    "x = Concatenate(axis=-1)([x11,x12,x21,x22,x31,x32,x41,x42])\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "preds = Dense(y_train.shape[1], activation='softmax',kernel_regularizer = regularizers.l2(1e-4))(x)\n",
    "\n",
    "model_12 = Model(in_x,preds)\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_12.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "hs_12_1 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_1st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1523 - acc: 0.9580 - val_loss: 0.4114 - val_acc: 0.8833\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1460 - acc: 0.9592 - val_loss: 0.4055 - val_acc: 0.8860\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1463 - acc: 0.9610 - val_loss: 0.4072 - val_acc: 0.8847\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.1436 - acc: 0.9604 - val_loss: 0.4048 - val_acc: 0.8829\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1416 - acc: 0.9609 - val_loss: 0.4005 - val_acc: 0.8838\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1381 - acc: 0.9643 - val_loss: 0.4015 - val_acc: 0.8816\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1336 - acc: 0.9665 - val_loss: 0.4003 - val_acc: 0.8873\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1389 - acc: 0.9640 - val_loss: 0.4006 - val_acc: 0.8873\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.1271 - acc: 0.9676 - val_loss: 0.3995 - val_acc: 0.8847\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.1337 - acc: 0.9646 - val_loss: 0.3988 - val_acc: 0.8838\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.1340 - acc: 0.9648 - val_loss: 0.3976 - val_acc: 0.8873\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1237 - acc: 0.9675 - val_loss: 0.4012 - val_acc: 0.8851\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1267 - acc: 0.9683 - val_loss: 0.4158 - val_acc: 0.8825\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 92s 10ms/step - loss: 0.1276 - acc: 0.9677 - val_loss: 0.3974 - val_acc: 0.8864\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1181 - acc: 0.9699 - val_loss: 0.4042 - val_acc: 0.8842\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1184 - acc: 0.9699 - val_loss: 0.4087 - val_acc: 0.8829\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.1278 - acc: 0.9660 - val_loss: 0.4117 - val_acc: 0.8833\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.1190 - acc: 0.9684 - val_loss: 0.4103 - val_acc: 0.8851\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1139 - acc: 0.9716 - val_loss: 0.4004 - val_acc: 0.8825\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.1251 - acc: 0.9673 - val_loss: 0.4023 - val_acc: 0.8838\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.1145 - acc: 0.9713 - val_loss: 0.4066 - val_acc: 0.8838\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.1119 - acc: 0.9720 - val_loss: 0.3999 - val_acc: 0.8878\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1211 - acc: 0.9694 - val_loss: 0.3951 - val_acc: 0.8913\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1119 - acc: 0.9707 - val_loss: 0.4073 - val_acc: 0.8895\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1138 - acc: 0.9722 - val_loss: 0.4114 - val_acc: 0.8833\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1026 - acc: 0.9728 - val_loss: 0.4064 - val_acc: 0.8820\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.1075 - acc: 0.9746 - val_loss: 0.4082 - val_acc: 0.8807\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 94s 10ms/step - loss: 0.1082 - acc: 0.9732 - val_loss: 0.4226 - val_acc: 0.8807\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.1068 - acc: 0.9746 - val_loss: 0.4127 - val_acc: 0.8873\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.1057 - acc: 0.9746 - val_loss: 0.4268 - val_acc: 0.8820\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1043 - acc: 0.9732 - val_loss: 0.4067 - val_acc: 0.8833\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.1134 - acc: 0.9733 - val_loss: 0.4120 - val_acc: 0.8864\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.1030 - acc: 0.9743 - val_loss: 0.4149 - val_acc: 0.8860\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.1114 - acc: 0.9714 - val_loss: 0.4143 - val_acc: 0.8860\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0994 - acc: 0.9769 - val_loss: 0.4163 - val_acc: 0.8878\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.1075 - acc: 0.9725 - val_loss: 0.4302 - val_acc: 0.8816\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.1005 - acc: 0.9750 - val_loss: 0.4129 - val_acc: 0.8856\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0975 - acc: 0.9770 - val_loss: 0.4155 - val_acc: 0.8833\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0997 - acc: 0.9766 - val_loss: 0.4225 - val_acc: 0.8869\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.1018 - acc: 0.9757 - val_loss: 0.4117 - val_acc: 0.8847\n"
     ]
    }
   ],
   "source": [
    "hs_12_2 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_2nd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0972 - acc: 0.9779 - val_loss: 0.4122 - val_acc: 0.8869\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0783 - acc: 0.9834 - val_loss: 0.4104 - val_acc: 0.8873\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0938 - acc: 0.9804 - val_loss: 0.4090 - val_acc: 0.8878\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0828 - acc: 0.9825 - val_loss: 0.4108 - val_acc: 0.8886\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0795 - acc: 0.9830 - val_loss: 0.4123 - val_acc: 0.8869\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0840 - acc: 0.9833 - val_loss: 0.4032 - val_acc: 0.8891\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0823 - acc: 0.9824 - val_loss: 0.4159 - val_acc: 0.8886\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0832 - acc: 0.9836 - val_loss: 0.4077 - val_acc: 0.8878\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0813 - acc: 0.9824 - val_loss: 0.4086 - val_acc: 0.8886\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0811 - acc: 0.9835 - val_loss: 0.4172 - val_acc: 0.8891\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0725 - acc: 0.9861 - val_loss: 0.4085 - val_acc: 0.8882\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0720 - acc: 0.9857 - val_loss: 0.4066 - val_acc: 0.8939\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0680 - acc: 0.9880 - val_loss: 0.4065 - val_acc: 0.8913\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0753 - acc: 0.9848 - val_loss: 0.4126 - val_acc: 0.8900\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0739 - acc: 0.9841 - val_loss: 0.4138 - val_acc: 0.8909\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0739 - acc: 0.9851 - val_loss: 0.4086 - val_acc: 0.8917\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0724 - acc: 0.9856 - val_loss: 0.4091 - val_acc: 0.8935\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0730 - acc: 0.9856 - val_loss: 0.4108 - val_acc: 0.8909\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0750 - acc: 0.9859 - val_loss: 0.4137 - val_acc: 0.8935\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0678 - acc: 0.9887 - val_loss: 0.4074 - val_acc: 0.8926\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0680 - acc: 0.9864 - val_loss: 0.3988 - val_acc: 0.8939\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0756 - acc: 0.9860 - val_loss: 0.4081 - val_acc: 0.8904\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0677 - acc: 0.9878 - val_loss: 0.4076 - val_acc: 0.8900\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0728 - acc: 0.9859 - val_loss: 0.4111 - val_acc: 0.8900\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0708 - acc: 0.9876 - val_loss: 0.4058 - val_acc: 0.8886\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0720 - acc: 0.9871 - val_loss: 0.4097 - val_acc: 0.8878\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0687 - acc: 0.9869 - val_loss: 0.4085 - val_acc: 0.8904\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0717 - acc: 0.9856 - val_loss: 0.4036 - val_acc: 0.8939\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0629 - acc: 0.9881 - val_loss: 0.4075 - val_acc: 0.8917\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0664 - acc: 0.9877 - val_loss: 0.4014 - val_acc: 0.8944\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0661 - acc: 0.9882 - val_loss: 0.4077 - val_acc: 0.8891\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0645 - acc: 0.9875 - val_loss: 0.4087 - val_acc: 0.8882\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0707 - acc: 0.9873 - val_loss: 0.4131 - val_acc: 0.8935\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0675 - acc: 0.9872 - val_loss: 0.4083 - val_acc: 0.8935\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0679 - acc: 0.9880 - val_loss: 0.4057 - val_acc: 0.8913\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0683 - acc: 0.9867 - val_loss: 0.4063 - val_acc: 0.8931\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0683 - acc: 0.9877 - val_loss: 0.4003 - val_acc: 0.8953\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0643 - acc: 0.9891 - val_loss: 0.4094 - val_acc: 0.8909\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0740 - acc: 0.9852 - val_loss: 0.3991 - val_acc: 0.8926\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0623 - acc: 0.9905 - val_loss: 0.4141 - val_acc: 0.8864\n"
     ]
    }
   ],
   "source": [
    "model_12 = load_model(\"TextCNN_option_7_2nd\")\n",
    "optimizer = optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_12.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "hs_12_3 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_3rd\")# 8953 37th 8944 30th 8939 12th 21th 28th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0684 - acc: 0.9874 - val_loss: 0.3978 - val_acc: 0.8931\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0750 - acc: 0.9866 - val_loss: 0.3985 - val_acc: 0.8975\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0706 - acc: 0.9856 - val_loss: 0.4021 - val_acc: 0.8962\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0650 - acc: 0.9885 - val_loss: 0.4032 - val_acc: 0.8944\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0676 - acc: 0.9878 - val_loss: 0.3975 - val_acc: 0.8939\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0660 - acc: 0.9876 - val_loss: 0.4053 - val_acc: 0.8931\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0616 - acc: 0.9886 - val_loss: 0.4114 - val_acc: 0.8926\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0607 - acc: 0.9892 - val_loss: 0.4009 - val_acc: 0.8957\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0675 - acc: 0.9862 - val_loss: 0.4071 - val_acc: 0.8966\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0622 - acc: 0.9888 - val_loss: 0.4035 - val_acc: 0.8997\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0630 - acc: 0.9882 - val_loss: 0.3994 - val_acc: 0.8970\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0649 - acc: 0.9873 - val_loss: 0.3994 - val_acc: 0.8970\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0636 - acc: 0.9885 - val_loss: 0.4015 - val_acc: 0.8975\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0608 - acc: 0.9890 - val_loss: 0.4034 - val_acc: 0.8944\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 93s 10ms/step - loss: 0.0565 - acc: 0.9918 - val_loss: 0.4000 - val_acc: 0.8970\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0618 - acc: 0.9897 - val_loss: 0.4056 - val_acc: 0.8931\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0610 - acc: 0.9904 - val_loss: 0.4008 - val_acc: 0.8962\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0638 - acc: 0.9887 - val_loss: 0.4159 - val_acc: 0.8913\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0613 - acc: 0.9896 - val_loss: 0.3992 - val_acc: 0.8939\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0601 - acc: 0.9898 - val_loss: 0.4009 - val_acc: 0.8948\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0628 - acc: 0.9890 - val_loss: 0.4077 - val_acc: 0.8957\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0621 - acc: 0.9897 - val_loss: 0.3995 - val_acc: 0.8979\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0577 - acc: 0.9918 - val_loss: 0.3965 - val_acc: 0.8970\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0569 - acc: 0.9907 - val_loss: 0.3954 - val_acc: 0.8948\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0629 - acc: 0.9896 - val_loss: 0.3968 - val_acc: 0.8970\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0608 - acc: 0.9892 - val_loss: 0.3990 - val_acc: 0.8922\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0637 - acc: 0.9894 - val_loss: 0.4017 - val_acc: 0.8944\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0609 - acc: 0.9899 - val_loss: 0.4028 - val_acc: 0.8970\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0609 - acc: 0.9896 - val_loss: 0.4028 - val_acc: 0.8957\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0586 - acc: 0.9916 - val_loss: 0.4022 - val_acc: 0.8953\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0627 - acc: 0.9899 - val_loss: 0.4016 - val_acc: 0.8957\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0595 - acc: 0.9905 - val_loss: 0.3992 - val_acc: 0.8966\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0564 - acc: 0.9914 - val_loss: 0.4065 - val_acc: 0.8953\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0643 - acc: 0.9887 - val_loss: 0.3981 - val_acc: 0.8975\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0583 - acc: 0.9904 - val_loss: 0.4015 - val_acc: 0.8939\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0617 - acc: 0.9888 - val_loss: 0.3994 - val_acc: 0.8984\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0591 - acc: 0.9892 - val_loss: 0.4022 - val_acc: 0.8953\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0601 - acc: 0.9893 - val_loss: 0.4048 - val_acc: 0.8948\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0589 - acc: 0.9903 - val_loss: 0.3978 - val_acc: 0.8984\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0642 - acc: 0.9894 - val_loss: 0.4026 - val_acc: 0.8944\n"
     ]
    }
   ],
   "source": [
    "hs_12_4 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_4th\")#8997 10th 8984 36th 39th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0585 - acc: 0.9907 - val_loss: 0.4056 - val_acc: 0.8939\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0594 - acc: 0.9912 - val_loss: 0.4001 - val_acc: 0.8922\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0584 - acc: 0.9912 - val_loss: 0.4056 - val_acc: 0.8931\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0580 - acc: 0.9908 - val_loss: 0.3987 - val_acc: 0.8948\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0575 - acc: 0.9908 - val_loss: 0.3975 - val_acc: 0.8948\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0549 - acc: 0.9913 - val_loss: 0.4026 - val_acc: 0.8948\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0553 - acc: 0.9924 - val_loss: 0.3998 - val_acc: 0.8957\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0606 - acc: 0.9899 - val_loss: 0.4128 - val_acc: 0.8939\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0569 - acc: 0.9920 - val_loss: 0.3949 - val_acc: 0.8966\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0607 - acc: 0.9906 - val_loss: 0.3949 - val_acc: 0.9006\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0572 - acc: 0.9911 - val_loss: 0.4007 - val_acc: 0.8966\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0568 - acc: 0.9904 - val_loss: 0.3922 - val_acc: 0.8997\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0558 - acc: 0.9919 - val_loss: 0.3922 - val_acc: 0.9028\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0567 - acc: 0.9913 - val_loss: 0.3945 - val_acc: 0.8997\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0600 - acc: 0.9894 - val_loss: 0.3917 - val_acc: 0.8988\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0548 - acc: 0.9913 - val_loss: 0.3931 - val_acc: 0.8992\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0577 - acc: 0.9906 - val_loss: 0.3959 - val_acc: 0.8997\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0566 - acc: 0.9916 - val_loss: 0.3938 - val_acc: 0.8992\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0609 - acc: 0.9906 - val_loss: 0.3987 - val_acc: 0.8992\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0545 - acc: 0.9914 - val_loss: 0.4039 - val_acc: 0.8962\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0545 - acc: 0.9914 - val_loss: 0.3958 - val_acc: 0.9010\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0579 - acc: 0.9907 - val_loss: 0.4001 - val_acc: 0.8966\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0541 - acc: 0.9915 - val_loss: 0.3940 - val_acc: 0.8966\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0561 - acc: 0.9909 - val_loss: 0.4083 - val_acc: 0.8944\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0606 - acc: 0.9902 - val_loss: 0.3960 - val_acc: 0.8984\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0544 - acc: 0.9917 - val_loss: 0.3973 - val_acc: 0.8975\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0564 - acc: 0.9923 - val_loss: 0.4005 - val_acc: 0.9001\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0549 - acc: 0.9911 - val_loss: 0.4001 - val_acc: 0.8992\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 93s 10ms/step - loss: 0.0600 - acc: 0.9890 - val_loss: 0.4016 - val_acc: 0.8975\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0562 - acc: 0.9909 - val_loss: 0.3972 - val_acc: 0.8957\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0539 - acc: 0.9924 - val_loss: 0.4101 - val_acc: 0.8939\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0556 - acc: 0.9923 - val_loss: 0.3997 - val_acc: 0.8988\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0550 - acc: 0.9917 - val_loss: 0.3961 - val_acc: 0.8992\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0554 - acc: 0.9917 - val_loss: 0.4005 - val_acc: 0.8975\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0576 - acc: 0.9904 - val_loss: 0.3998 - val_acc: 0.8966\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0544 - acc: 0.9912 - val_loss: 0.3951 - val_acc: 0.8970\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0551 - acc: 0.9920 - val_loss: 0.3974 - val_acc: 0.8975\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0576 - acc: 0.9906 - val_loss: 0.4013 - val_acc: 0.8922\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0581 - acc: 0.9904 - val_loss: 0.3914 - val_acc: 0.8975\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0515 - acc: 0.9931 - val_loss: 0.3946 - val_acc: 0.8944\n"
     ]
    }
   ],
   "source": [
    "hs_12_5 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_5th\") #9028 13th,9010 21th,9006 10th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0544 - acc: 0.9913 - val_loss: 0.4016 - val_acc: 0.8948\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0537 - acc: 0.9928 - val_loss: 0.4088 - val_acc: 0.8975\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0587 - acc: 0.9916 - val_loss: 0.4008 - val_acc: 0.8944\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0557 - acc: 0.9898 - val_loss: 0.4046 - val_acc: 0.8939\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0542 - acc: 0.9912 - val_loss: 0.4028 - val_acc: 0.8939\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0568 - acc: 0.9916 - val_loss: 0.3972 - val_acc: 0.8962\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0524 - acc: 0.9926 - val_loss: 0.4044 - val_acc: 0.8944\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0541 - acc: 0.9923 - val_loss: 0.4064 - val_acc: 0.8957\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0563 - acc: 0.9912 - val_loss: 0.4045 - val_acc: 0.8957\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0514 - acc: 0.9920 - val_loss: 0.3951 - val_acc: 0.8953\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0575 - acc: 0.9916 - val_loss: 0.3994 - val_acc: 0.8975\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0538 - acc: 0.9930 - val_loss: 0.3978 - val_acc: 0.8957\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 92s 10ms/step - loss: 0.0557 - acc: 0.9912 - val_loss: 0.3994 - val_acc: 0.8966\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0524 - acc: 0.9917 - val_loss: 0.4019 - val_acc: 0.8975\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0538 - acc: 0.9916 - val_loss: 0.4079 - val_acc: 0.8939\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0583 - acc: 0.9911 - val_loss: 0.4076 - val_acc: 0.8944\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0555 - acc: 0.9901 - val_loss: 0.4016 - val_acc: 0.8948\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0574 - acc: 0.9908 - val_loss: 0.4062 - val_acc: 0.8979\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0498 - acc: 0.9931 - val_loss: 0.3967 - val_acc: 0.8948\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0549 - acc: 0.9920 - val_loss: 0.4024 - val_acc: 0.8979\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0537 - acc: 0.9922 - val_loss: 0.4071 - val_acc: 0.8975\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0552 - acc: 0.9918 - val_loss: 0.3957 - val_acc: 0.8948\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0553 - acc: 0.9917 - val_loss: 0.4029 - val_acc: 0.8909\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0578 - acc: 0.9894 - val_loss: 0.4077 - val_acc: 0.8922\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0553 - acc: 0.9913 - val_loss: 0.4003 - val_acc: 0.8948\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0559 - acc: 0.9924 - val_loss: 0.4006 - val_acc: 0.8926\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0489 - acc: 0.9934 - val_loss: 0.4062 - val_acc: 0.8944\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0511 - acc: 0.9929 - val_loss: 0.4009 - val_acc: 0.8944\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0531 - acc: 0.9914 - val_loss: 0.4040 - val_acc: 0.8953\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0516 - acc: 0.9922 - val_loss: 0.3981 - val_acc: 0.8957\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0535 - acc: 0.9923 - val_loss: 0.4055 - val_acc: 0.8970\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0541 - acc: 0.9906 - val_loss: 0.4054 - val_acc: 0.8988\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0536 - acc: 0.9930 - val_loss: 0.4065 - val_acc: 0.9006\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0510 - acc: 0.9933 - val_loss: 0.4100 - val_acc: 0.8939\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0510 - acc: 0.9931 - val_loss: 0.4023 - val_acc: 0.8979\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0570 - acc: 0.9913 - val_loss: 0.4030 - val_acc: 0.8979\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0487 - acc: 0.9937 - val_loss: 0.4061 - val_acc: 0.8997\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0560 - acc: 0.9917 - val_loss: 0.3998 - val_acc: 0.8979\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0519 - acc: 0.9914 - val_loss: 0.3955 - val_acc: 0.8984\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0500 - acc: 0.9933 - val_loss: 0.3974 - val_acc: 0.9010\n"
     ]
    }
   ],
   "source": [
    "hs_12_6 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_6th\") #9010 40th 9006 33th "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0482 - acc: 0.9936 - val_loss: 0.3966 - val_acc: 0.9001\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0506 - acc: 0.9928 - val_loss: 0.3969 - val_acc: 0.9006\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0489 - acc: 0.9931 - val_loss: 0.3937 - val_acc: 0.9010\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0538 - acc: 0.9911 - val_loss: 0.3939 - val_acc: 0.9001\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0532 - acc: 0.9916 - val_loss: 0.3974 - val_acc: 0.8992\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0470 - acc: 0.9940 - val_loss: 0.3998 - val_acc: 0.8988\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0480 - acc: 0.9944 - val_loss: 0.3937 - val_acc: 0.8979\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0453 - acc: 0.9945 - val_loss: 0.3963 - val_acc: 0.8984\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0476 - acc: 0.9941 - val_loss: 0.3881 - val_acc: 0.9001\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0473 - acc: 0.9944 - val_loss: 0.3955 - val_acc: 0.8975\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0467 - acc: 0.9944 - val_loss: 0.3996 - val_acc: 0.9015\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0459 - acc: 0.9944 - val_loss: 0.4019 - val_acc: 0.8975\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0504 - acc: 0.9928 - val_loss: 0.3977 - val_acc: 0.8970\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0465 - acc: 0.9943 - val_loss: 0.3926 - val_acc: 0.9023\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0447 - acc: 0.9944 - val_loss: 0.3928 - val_acc: 0.8992\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0451 - acc: 0.9946 - val_loss: 0.3924 - val_acc: 0.8988\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0448 - acc: 0.9950 - val_loss: 0.3923 - val_acc: 0.8966\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0459 - acc: 0.9945 - val_loss: 0.3918 - val_acc: 0.9028\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0483 - acc: 0.9922 - val_loss: 0.3953 - val_acc: 0.8979\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0464 - acc: 0.9933 - val_loss: 0.3922 - val_acc: 0.9010\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0474 - acc: 0.9937 - val_loss: 0.3917 - val_acc: 0.9015\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0436 - acc: 0.9950 - val_loss: 0.3930 - val_acc: 0.9001\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0469 - acc: 0.9935 - val_loss: 0.3925 - val_acc: 0.8992\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0460 - acc: 0.9939 - val_loss: 0.3936 - val_acc: 0.8992\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 83s 9ms/step - loss: 0.0446 - acc: 0.9945 - val_loss: 0.3903 - val_acc: 0.9023\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0456 - acc: 0.9944 - val_loss: 0.3966 - val_acc: 0.9028\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0453 - acc: 0.9945 - val_loss: 0.3958 - val_acc: 0.9006\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0490 - acc: 0.9926 - val_loss: 0.3925 - val_acc: 0.9028\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 99s 11ms/step - loss: 0.0453 - acc: 0.9950 - val_loss: 0.3957 - val_acc: 0.9001\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 112s 12ms/step - loss: 0.0446 - acc: 0.9940 - val_loss: 0.3939 - val_acc: 0.9006\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 112s 12ms/step - loss: 0.0436 - acc: 0.9956 - val_loss: 0.3925 - val_acc: 0.9010\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 113s 13ms/step - loss: 0.0468 - acc: 0.9939 - val_loss: 0.3945 - val_acc: 0.8992\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0450 - acc: 0.9952 - val_loss: 0.3886 - val_acc: 0.9032\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0431 - acc: 0.9952 - val_loss: 0.3896 - val_acc: 0.9019\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0426 - acc: 0.9952 - val_loss: 0.3892 - val_acc: 0.9019\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0479 - acc: 0.9939 - val_loss: 0.3968 - val_acc: 0.8984\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0475 - acc: 0.9933 - val_loss: 0.3955 - val_acc: 0.8979\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0450 - acc: 0.9940 - val_loss: 0.3978 - val_acc: 0.9001\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0466 - acc: 0.9931 - val_loss: 0.3945 - val_acc: 0.9001\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0425 - acc: 0.9951 - val_loss: 0.3930 - val_acc: 0.8992\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.0002, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_12.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "hs_12_7 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_7th\") #9032 33th 9028 18th 26th  28th 9023 14th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 94s 10ms/step - loss: 0.0444 - acc: 0.9951 - val_loss: 0.3960 - val_acc: 0.9010\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0438 - acc: 0.9940 - val_loss: 0.3929 - val_acc: 0.9006\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0427 - acc: 0.9945 - val_loss: 0.3925 - val_acc: 0.9010\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0436 - acc: 0.9944 - val_loss: 0.3875 - val_acc: 0.9001\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0418 - acc: 0.9951 - val_loss: 0.3893 - val_acc: 0.8992\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0433 - acc: 0.9952 - val_loss: 0.3921 - val_acc: 0.9001\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0455 - acc: 0.9941 - val_loss: 0.3887 - val_acc: 0.9028\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0449 - acc: 0.9950 - val_loss: 0.3889 - val_acc: 0.8992\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0425 - acc: 0.9960 - val_loss: 0.3878 - val_acc: 0.9010\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0428 - acc: 0.9951 - val_loss: 0.3869 - val_acc: 0.9010\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0446 - acc: 0.9940 - val_loss: 0.3934 - val_acc: 0.9001\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0415 - acc: 0.9960 - val_loss: 0.3913 - val_acc: 0.9006\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0420 - acc: 0.9962 - val_loss: 0.3904 - val_acc: 0.9023\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0424 - acc: 0.9945 - val_loss: 0.3906 - val_acc: 0.9001\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0424 - acc: 0.9958 - val_loss: 0.3915 - val_acc: 0.8988\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0421 - acc: 0.9951 - val_loss: 0.3895 - val_acc: 0.9006\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0440 - acc: 0.9948 - val_loss: 0.3869 - val_acc: 0.9023\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0423 - acc: 0.9947 - val_loss: 0.3909 - val_acc: 0.8992\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0441 - acc: 0.9944 - val_loss: 0.3899 - val_acc: 0.9015\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0433 - acc: 0.9952 - val_loss: 0.3868 - val_acc: 0.9006\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0439 - acc: 0.9940 - val_loss: 0.3893 - val_acc: 0.8997\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0454 - acc: 0.9941 - val_loss: 0.3912 - val_acc: 0.8997\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0422 - acc: 0.9951 - val_loss: 0.3913 - val_acc: 0.9019\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0403 - acc: 0.9956 - val_loss: 0.3884 - val_acc: 0.9015\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0431 - acc: 0.9950 - val_loss: 0.3895 - val_acc: 0.9019\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0429 - acc: 0.9948 - val_loss: 0.3914 - val_acc: 0.9001\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0415 - acc: 0.9961 - val_loss: 0.3926 - val_acc: 0.9006\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0417 - acc: 0.9949 - val_loss: 0.3928 - val_acc: 0.9010\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0408 - acc: 0.9952 - val_loss: 0.3916 - val_acc: 0.9019\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0418 - acc: 0.9949 - val_loss: 0.3893 - val_acc: 0.9001\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0426 - acc: 0.9951 - val_loss: 0.3883 - val_acc: 0.9015\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 94s 10ms/step - loss: 0.0435 - acc: 0.9955 - val_loss: 0.3915 - val_acc: 0.9010\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0427 - acc: 0.9950 - val_loss: 0.3901 - val_acc: 0.8979\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0388 - acc: 0.9962 - val_loss: 0.3913 - val_acc: 0.9001\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 93s 10ms/step - loss: 0.0413 - acc: 0.9956 - val_loss: 0.3900 - val_acc: 0.9015\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0402 - acc: 0.9964 - val_loss: 0.3880 - val_acc: 0.9023\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0444 - acc: 0.9948 - val_loss: 0.3928 - val_acc: 0.9019\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0424 - acc: 0.9947 - val_loss: 0.3904 - val_acc: 0.9023\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0414 - acc: 0.9957 - val_loss: 0.3930 - val_acc: 0.9015\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0421 - acc: 0.9951 - val_loss: 0.3910 - val_acc: 0.8997\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_12.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "\n",
    "hs_12_8 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_8th\") #9028 7th 9023 13th 17th 36th 38th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0404 - acc: 0.9955 - val_loss: 0.3908 - val_acc: 0.9032\n",
      "Epoch 2/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0408 - acc: 0.9960 - val_loss: 0.3900 - val_acc: 0.9032\n",
      "Epoch 3/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0393 - acc: 0.9956 - val_loss: 0.3893 - val_acc: 0.9023\n",
      "Epoch 4/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0409 - acc: 0.9956 - val_loss: 0.3884 - val_acc: 0.9046\n",
      "Epoch 5/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0402 - acc: 0.9965 - val_loss: 0.3886 - val_acc: 0.9019\n",
      "Epoch 6/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0461 - acc: 0.9944 - val_loss: 0.3900 - val_acc: 0.9032\n",
      "Epoch 7/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0421 - acc: 0.9960 - val_loss: 0.3884 - val_acc: 0.9019\n",
      "Epoch 8/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0415 - acc: 0.9959 - val_loss: 0.3884 - val_acc: 0.9041\n",
      "Epoch 9/40\n",
      "9051/9051 [==============================] - 89s 10ms/step - loss: 0.0414 - acc: 0.9945 - val_loss: 0.3895 - val_acc: 0.9032\n",
      "Epoch 10/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0417 - acc: 0.9948 - val_loss: 0.3916 - val_acc: 0.9032\n",
      "Epoch 11/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0442 - acc: 0.9937 - val_loss: 0.3909 - val_acc: 0.9023\n",
      "Epoch 12/40\n",
      "9051/9051 [==============================] - 91s 10ms/step - loss: 0.0455 - acc: 0.9945 - val_loss: 0.3907 - val_acc: 0.9032\n",
      "Epoch 13/40\n",
      "9051/9051 [==============================] - 92s 10ms/step - loss: 0.0392 - acc: 0.9961 - val_loss: 0.3913 - val_acc: 0.9015\n",
      "Epoch 14/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0401 - acc: 0.9961 - val_loss: 0.3912 - val_acc: 0.9010\n",
      "Epoch 15/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0437 - acc: 0.9950 - val_loss: 0.3897 - val_acc: 0.9010\n",
      "Epoch 16/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0404 - acc: 0.9957 - val_loss: 0.3897 - val_acc: 0.9019\n",
      "Epoch 17/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0425 - acc: 0.9948 - val_loss: 0.3918 - val_acc: 0.9015\n",
      "Epoch 18/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0405 - acc: 0.9956 - val_loss: 0.3919 - val_acc: 0.9015\n",
      "Epoch 19/40\n",
      "9051/9051 [==============================] - 86s 10ms/step - loss: 0.0423 - acc: 0.9951 - val_loss: 0.3896 - val_acc: 0.9046\n",
      "Epoch 20/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0411 - acc: 0.9958 - val_loss: 0.3900 - val_acc: 0.9037\n",
      "Epoch 21/40\n",
      "9051/9051 [==============================] - 95s 11ms/step - loss: 0.0417 - acc: 0.9952 - val_loss: 0.3898 - val_acc: 0.9019\n",
      "Epoch 22/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0423 - acc: 0.9951 - val_loss: 0.3903 - val_acc: 0.9023\n",
      "Epoch 23/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0375 - acc: 0.9967 - val_loss: 0.3912 - val_acc: 0.9015\n",
      "Epoch 24/40\n",
      "9051/9051 [==============================] - 86s 9ms/step - loss: 0.0411 - acc: 0.9955 - val_loss: 0.3902 - val_acc: 0.9019\n",
      "Epoch 25/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0431 - acc: 0.9951 - val_loss: 0.3899 - val_acc: 0.9023\n",
      "Epoch 26/40\n",
      "9051/9051 [==============================] - 90s 10ms/step - loss: 0.0390 - acc: 0.9959 - val_loss: 0.3899 - val_acc: 0.9023\n",
      "Epoch 27/40\n",
      "9051/9051 [==============================] - 88s 10ms/step - loss: 0.0409 - acc: 0.9951 - val_loss: 0.3909 - val_acc: 0.9015\n",
      "Epoch 28/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0402 - acc: 0.9969 - val_loss: 0.3900 - val_acc: 0.9023\n",
      "Epoch 29/40\n",
      "9051/9051 [==============================] - 85s 9ms/step - loss: 0.0408 - acc: 0.9956 - val_loss: 0.3902 - val_acc: 0.9037\n",
      "Epoch 30/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0410 - acc: 0.9955 - val_loss: 0.3921 - val_acc: 0.9001\n",
      "Epoch 31/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0420 - acc: 0.9947 - val_loss: 0.3906 - val_acc: 0.9019\n",
      "Epoch 32/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0415 - acc: 0.9948 - val_loss: 0.3891 - val_acc: 0.9023\n",
      "Epoch 33/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0426 - acc: 0.9944 - val_loss: 0.3899 - val_acc: 0.9019\n",
      "Epoch 34/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0430 - acc: 0.9951 - val_loss: 0.3901 - val_acc: 0.9032\n",
      "Epoch 35/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0400 - acc: 0.9948 - val_loss: 0.3904 - val_acc: 0.9032\n",
      "Epoch 36/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0380 - acc: 0.9955 - val_loss: 0.3892 - val_acc: 0.9019\n",
      "Epoch 37/40\n",
      "9051/9051 [==============================] - 87s 10ms/step - loss: 0.0413 - acc: 0.9947 - val_loss: 0.3889 - val_acc: 0.9019\n",
      "Epoch 38/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0401 - acc: 0.9957 - val_loss: 0.3901 - val_acc: 0.9019\n",
      "Epoch 39/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0419 - acc: 0.9951 - val_loss: 0.3890 - val_acc: 0.9019\n",
      "Epoch 40/40\n",
      "9051/9051 [==============================] - 84s 9ms/step - loss: 0.0392 - acc: 0.9957 - val_loss: 0.3895 - val_acc: 0.9015\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None)#'rmsprop', decay=0.1\n",
    "model_12.compile(loss = 'categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "\n",
    "hs_12_9 = model_12.fit(X_train,y_train,batch_size = 128,epochs = 40,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save(\"TextCNN_option_7_9th\") #9046 4th 19th 9041 8th 9032 n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: the sixth update, i.e. using glove.840B.300d, is in another notebook \"Model_3-TextCNN-Glove.840B.300d-submit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize improvements brought by the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = load_model(\"TextCNN_model_1\") # the original model\n",
    "model_1 = load_model(\"Text_CNN_option_4_2nd\") # the first update\n",
    "model_2 = load_model('TextCNN_option_5_3rd') # the second update\n",
    "model_3 = load_model(\"TextCNN_option_8_10th\") # the third update\n",
    "model_4 = load_model(\"TextCNN_option_10_9th\") # the fourth update\n",
    "model_5 = load_model(\"TextCNN_option_7_9th\") # the fifth update\n",
    "model_6 = load_model(\"Model_3-TextCNN-Glove.840B-1e-5-4\")# the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,X1,y1,X2,y2):\n",
    "    acc1 = model.evaluate(X1,y1)[1]\n",
    "    acc2 = model.evaluate(X2,y2)[1]\n",
    "    acc_avg = acc1*0.4 + acc2*0.6\n",
    "\n",
    "    return acc1, acc2, acc_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012/3012 [==============================] - 3s 1ms/step\n",
      "4520/4520 [==============================] - 5s 1ms/step\n",
      "0.7785524568393094 0.7803097345132743 0.7796068234436884\n"
     ]
    }
   ],
   "source": [
    "#model_0 \n",
    "acc1_m0,acc2_m0,acc_avg_m0 = test(model_0,X_test_1,y_test_1,X_test_2,y_test_2)\n",
    "print (acc1_m0,acc2_m0,acc_avg_m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012/3012 [==============================] - 3s 1ms/step\n",
      "4520/4520 [==============================] - 5s 1ms/step\n",
      "0.7825365205843293 0.7898230088495575 0.7869084135434662\n"
     ]
    }
   ],
   "source": [
    "#model_1 \n",
    "acc1_m1,acc2_m1,acc_avg_m1 = test(model_1,X_test_1,y_test_1,X_test_2,y_test_2)\n",
    "\n",
    "print (acc1_m1,acc2_m1,acc_avg_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012/3012 [==============================] - 6s 2ms/step\n",
      "4520/4520 [==============================] - 8s 2ms/step\n",
      "0.7908366533864541 0.8042035398230089 0.798856785248387\n"
     ]
    }
   ],
   "source": [
    "#model_2\n",
    "acc1_m2,acc2_m2,acc_avg_m2 = test(model_2,X_test_1,y_test_1,X_test_2,y_test_2)\n",
    "\n",
    "print (acc1_m2,acc2_m2,acc_avg_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012/3012 [==============================] - 6s 2ms/step\n",
      "4520/4520 [==============================] - 8s 2ms/step\n",
      "0.8094289508632138 0.8139380530973451 0.8121344122036926\n"
     ]
    }
   ],
   "source": [
    "#model_3\n",
    "acc1_m3,acc2_m3,acc_avg_m3 = test(model_3,X_test_1,y_test_1,X_test_2,y_test_2)\n",
    "\n",
    "print (acc1_m3,acc2_m3,acc_avg_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012/3012 [==============================] - 6s 2ms/step\n",
      "4520/4520 [==============================] - 8s 2ms/step\n",
      "0.8150730411686588 0.8303097345132744 0.8242150571754281\n"
     ]
    }
   ],
   "source": [
    "#model_4\n",
    "acc1_m4,acc2_m4,acc_avg_m4 = test(model_4,X_test_1,y_test_1,X_test_2,y_test_2)\n",
    "\n",
    "print (acc1_m4,acc2_m4,acc_avg_m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012/3012 [==============================] - 5s 2ms/step\n",
      "4520/4520 [==============================] - 8s 2ms/step\n",
      "0.8144090305444888 0.8305309734513274 0.824082196288592\n"
     ]
    }
   ],
   "source": [
    "#model_5\n",
    "acc1_m5,acc2_m5,acc_avg_m5 = test(model_5,X_test_1,y_test_1,X_test_2,y_test_2)\n",
    "\n",
    "print (acc1_m5,acc2_m5,acc_avg_m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.850265604249668 0.856858407079646 0.8542212859476548\n"
     ]
    }
   ],
   "source": [
    "# model_6\n",
    "acc1_m6,acc2_m6,acc_avg_m6 = (0.850265604249668, 0.856858407079646, 0.8542212859476548)\n",
    "print (acc1_m6,acc2_m6,acc_avg_m6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_value = range(7)\n",
    "y_value = [acc_avg_m0,acc_avg_m1,acc_avg_m2,acc_avg_m3,acc_avg_m4,acc_avg_m5,acc_avg_m6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a2a3ff278>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a299b0390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "fig = plt.figure(figsize = (9,6))\n",
    "labels = ['Original Model','GN','more filters','L2','n-gram','GlobalAvgPooling','840B']\n",
    "colors = ['r','y','b','c','g','purple','k']\n",
    "\n",
    "for i in range(7):\n",
    "    plt.bar(x_value[i],y_value[i],color = colors[i],alpha = 0.8)\n",
    "\n",
    "plt.plot(x_value,y_value,color = 'c')\n",
    "plt.title(\"Performance Improvements\",fontsize = 16)\n",
    "plt.xticks([0.4,1.4,2.4,3.4,4.4,5.4,6.4],labels)#\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
